{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/py3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986848"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = open(\"theLordOfTheRings.txt\")\n",
    "data = txt.read()\n",
    "data = data.lower()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the lord of the rings 1. concerning hobbits this book is largely concerned with hobbits, and from its pages a reader may discover much of their character and a little of their history.  further inform'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the lord of the rings 1. concerning hobbits this book is largely concerned with hobbits, and from its pages a reader may discover much of their character and a little of their history.',\n",
       " 'further information will also be found in the selection from the red book of westmarch that has already been published, under the title of the hobbit.',\n",
       " 'that story was derived from the earlier chapters of the red book, composed by bilbo himself, the first hobbit to become famous in the world at large, and called by him there and back again, since they told of his journey into the east and his return: an adventure which later involved all the hobbits in the great events of that age that are here related.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(data)\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218302"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(data)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['further',\n",
       "  'information',\n",
       "  'will',\n",
       "  'also',\n",
       "  'be',\n",
       "  'found',\n",
       "  'in',\n",
       "  'the',\n",
       "  'selection',\n",
       "  'from',\n",
       "  'the',\n",
       "  'red',\n",
       "  'book',\n",
       "  'of',\n",
       "  'westmarch',\n",
       "  'that',\n",
       "  'has',\n",
       "  'already',\n",
       "  'been',\n",
       "  'published',\n",
       "  ',',\n",
       "  'under',\n",
       "  'the',\n",
       "  'title',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hobbit',\n",
       "  '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_break_down = [word_tokenize(sentence) for sentence in sentences ]\n",
    "len(sentences_break_down)\n",
    "sentences_break_down[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences=sentences_break_down, \n",
    "        sg=1,\n",
    "        size=emb_dim,\n",
    "        window=5,\n",
    "        alpha=0.0005,\n",
    "        min_count=1,\n",
    "        workers=8,\n",
    "        batch_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37466700, 54575500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_len = len(w2v.wv.vocab)\n",
    "w2v.train(sentences=sentences_break_down, total_words=w2v_len, epochs=250, start_alpha=0.0005, end_alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gandalf', 1.0),\n",
       " ('strider', 0.9722239971160889),\n",
       " ('elrond', 0.9534536004066467),\n",
       " ('aragorn', 0.9465899467468262),\n",
       " ('boromir', 0.9435869455337524),\n",
       " ('legolas', 0.9333523511886597),\n",
       " ('haldir', 0.9300433397293091),\n",
       " ('frodo', 0.9268338084220886),\n",
       " ('glorfindel', 0.923527181148529),\n",
       " ('gildor', 0.9227961301803589)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similar_by_vector(w2v.wv.get_vector('gandalf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heart', 0.8881295919418335),\n",
       " ('gollum', 0.8830049633979797),\n",
       " ('account', 0.8823745250701904),\n",
       " ('thing', 0.8781375885009766),\n",
       " ('put', 0.8779300451278687),\n",
       " ('sauron', 0.8779194951057434),\n",
       " ('mind', 0.8778856992721558),\n",
       " ('desire', 0.877754807472229),\n",
       " ('tale', 0.8759180307388306),\n",
       " ('party', 0.8758673667907715)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive='ring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_dict = w2v.wv.vocab\n",
    "type(w2v_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_keys = np.array(list(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'lord', 'of', 'rings', '1.', 'concerning', 'hobbits', 'this',\n",
       "       'book', 'is'],\n",
       "      dtype='<U22')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_keys[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8555 5287 6028 7081  529 2022 4474 8606 1382 4806]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "w2v_keys_encoded = label_encoder.fit_transform(w2v_keys)\n",
    "print(w2v_keys_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "w2v_keys_encoded = w2v_keys_encoded.reshape(len(w2v_keys_encoded),1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(w2v_keys_encoded)\n",
    "print(onehot_encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE']\n"
     ]
    }
   ],
   "source": [
    "#inverted = label_encoder.inverse_transform([np.argmax(onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_group = 8\n",
    "random_range = int(len(words)/timestep_group) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(batch_size):\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    index = np.random.randint(0,random_range,batch_size)\n",
    "    #print(\"index: \",index)\n",
    "    for i in range(batch_size):\n",
    "        temp = []\n",
    "        for j in range(timestep_group-1):\n",
    "            #print(j,\"Get word:\",words[index[i]+j])\n",
    "            temp.append(w2v.wv.get_vector(words[index[i]+j]))\n",
    "        X_batch.append(temp)\n",
    "        #print(\"Label is:\", words[index[i]+timestep_group-1])\n",
    "        temp = label_encoder.transform([words[index[i]+timestep_group-1]])\n",
    "        tempv = onehot_encoder.transform(temp.reshape(1,1))\n",
    "        Y_batch.append(tempv[0])\n",
    "    return np.array(X_batch),np.array(Y_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3\n",
    "input_size = emb_dim\n",
    "timestep_size = timestep_group - 1 \n",
    "hidden_size = 1024\n",
    "layer_num = 2 \n",
    "class_num = w2v_len\n",
    "cell_type = \"lstm\"\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 7, emb_dim])\n",
    "y_input = tf.placeholder(tf.float32, [None, class_num])\n",
    "batch_size = tf.placeholder(tf.int32, [])\n",
    "keep_prob = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(cell_type,num_nodes,keep_prob):\n",
    "    # assert(cell_type in [\"lstm\",\"block_lstm\"],\"Wrong cell type\")\n",
    "    if cell_type == \"lstm\":\n",
    "        cell = rnn.BasicLSTMCell(num_nodes)\n",
    "    else:\n",
    "        cell = rnn.LSTMBlockCell(num_nodes)\n",
    "    cell = rnn.DropoutWrapper(cell,output_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "mlstm_cell = rnn.MultiRNNCell([lstm_cell(cell_type,hidden_size,keep_prob) for _ in range(layer_num)],state_is_tuple=True)\n",
    "init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = list()\n",
    "state = init_state\n",
    "\n",
    "with tf.variable_scope('RNN'):\n",
    "    for timestep in range(timestep_size):\n",
    "        (cell_output,state) = mlstm_cell(X[:,timestep,:],state)\n",
    "        outputs.append(cell_output)\n",
    "h_state = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([hidden_size,class_num],stddev=0.1),dtype=tf.float32)\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[class_num]),dtype=tf.float32)\n",
    "y_pre = tf.nn.softmax(tf.matmul(h_state,W) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = - tf.reduce_mean(y_input * tf.log(y_pre))\n",
    "train_op = tf.train.AdamOptimizer(alpha).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(y_pre,1),tf.argmax(y_input,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10,train cost=0.000825,acc=0.060000\n",
      "step 20,train cost=0.000770,acc=0.040000\n",
      "step 30,train cost=0.000775,acc=0.030000\n",
      "step 40,train cost=0.000708,acc=0.080000\n",
      "step 50,train cost=0.000693,acc=0.080000\n",
      "step 60,train cost=0.000708,acc=0.060000\n",
      "step 70,train cost=0.000659,acc=0.090000\n",
      "step 80,train cost=0.000672,acc=0.080000\n",
      "step 90,train cost=0.000686,acc=0.060000\n",
      "step 100,train cost=0.000716,acc=0.030000\n",
      "step 110,train cost=0.000675,acc=0.040000\n",
      "step 120,train cost=0.000620,acc=0.040000\n",
      "step 130,train cost=0.000708,acc=0.060000\n",
      "step 140,train cost=0.000629,acc=0.050000\n",
      "step 150,train cost=0.000670,acc=0.040000\n",
      "step 160,train cost=0.000651,acc=0.050000\n",
      "step 170,train cost=0.000625,acc=0.050000\n",
      "step 180,train cost=0.000642,acc=0.060000\n",
      "step 190,train cost=0.000641,acc=0.040000\n",
      "step 200,train cost=0.000622,acc=0.060000\n",
      "step 210,train cost=0.000663,acc=0.060000\n",
      "step 220,train cost=0.000642,acc=0.050000\n",
      "step 230,train cost=0.000714,acc=0.030000\n",
      "step 240,train cost=0.000608,acc=0.040000\n",
      "step 250,train cost=0.000674,acc=0.040000\n",
      "step 260,train cost=0.000633,acc=0.030000\n",
      "step 270,train cost=0.000635,acc=0.090000\n",
      "step 280,train cost=0.000631,acc=0.090000\n",
      "step 290,train cost=0.000668,acc=0.090000\n",
      "step 300,train cost=0.000655,acc=0.010000\n",
      "step 310,train cost=0.000660,acc=0.040000\n",
      "step 320,train cost=0.000640,acc=0.010000\n",
      "step 330,train cost=0.000645,acc=0.050000\n",
      "step 340,train cost=0.000624,acc=0.060000\n",
      "step 350,train cost=0.000626,acc=0.060000\n",
      "step 360,train cost=0.000611,acc=0.120000\n",
      "step 370,train cost=0.000626,acc=0.080000\n",
      "step 380,train cost=0.000637,acc=0.070000\n",
      "step 390,train cost=0.000605,acc=0.050000\n",
      "step 400,train cost=0.000608,acc=0.040000\n",
      "step 410,train cost=0.000592,acc=0.060000\n",
      "step 420,train cost=0.000621,acc=0.070000\n",
      "step 430,train cost=0.000613,acc=0.100000\n",
      "step 440,train cost=0.000630,acc=0.100000\n",
      "step 450,train cost=0.000600,acc=0.060000\n",
      "step 460,train cost=0.000701,acc=0.050000\n",
      "step 470,train cost=0.000550,acc=0.100000\n",
      "step 480,train cost=0.000607,acc=0.050000\n",
      "step 490,train cost=0.000582,acc=0.120000\n",
      "step 500,train cost=0.000667,acc=0.050000\n",
      "step 510,train cost=0.000614,acc=0.100000\n",
      "step 520,train cost=0.000603,acc=0.140000\n",
      "step 530,train cost=0.000628,acc=0.090000\n",
      "step 540,train cost=0.000607,acc=0.080000\n",
      "step 550,train cost=0.000607,acc=0.100000\n",
      "step 560,train cost=0.000636,acc=0.070000\n",
      "step 570,train cost=0.000649,acc=0.100000\n",
      "step 580,train cost=0.000614,acc=0.110000\n",
      "step 590,train cost=0.000621,acc=0.100000\n",
      "step 600,train cost=0.000577,acc=0.070000\n",
      "step 610,train cost=0.000568,acc=0.100000\n",
      "step 620,train cost=0.000615,acc=0.070000\n",
      "step 630,train cost=0.000586,acc=0.100000\n",
      "step 640,train cost=0.000608,acc=0.070000\n",
      "step 650,train cost=0.000586,acc=0.090000\n",
      "step 660,train cost=0.000617,acc=0.050000\n",
      "step 670,train cost=0.000603,acc=0.090000\n",
      "step 680,train cost=0.000564,acc=0.150000\n",
      "step 690,train cost=0.000574,acc=0.110000\n",
      "step 700,train cost=0.000594,acc=0.100000\n",
      "step 710,train cost=0.000581,acc=0.090000\n",
      "step 720,train cost=0.000577,acc=0.070000\n",
      "step 730,train cost=0.000616,acc=0.090000\n",
      "step 740,train cost=0.000622,acc=0.100000\n",
      "step 750,train cost=0.000533,acc=0.140000\n",
      "step 760,train cost=0.000604,acc=0.080000\n",
      "step 770,train cost=0.000562,acc=0.110000\n",
      "step 780,train cost=0.000562,acc=0.130000\n",
      "step 790,train cost=0.000507,acc=0.140000\n",
      "step 800,train cost=0.000503,acc=0.170000\n",
      "step 810,train cost=0.000622,acc=0.110000\n",
      "step 820,train cost=0.000572,acc=0.130000\n",
      "step 830,train cost=0.000596,acc=0.080000\n",
      "step 840,train cost=0.000574,acc=0.150000\n",
      "step 850,train cost=0.000574,acc=0.150000\n",
      "step 860,train cost=0.000575,acc=0.070000\n",
      "step 870,train cost=0.000595,acc=0.060000\n",
      "step 880,train cost=0.000580,acc=0.140000\n",
      "step 890,train cost=0.000545,acc=0.110000\n",
      "step 900,train cost=0.000590,acc=0.100000\n",
      "step 910,train cost=0.000591,acc=0.150000\n",
      "step 920,train cost=0.000551,acc=0.170000\n",
      "step 930,train cost=0.000544,acc=0.190000\n",
      "step 940,train cost=0.000604,acc=0.110000\n",
      "step 950,train cost=0.000574,acc=0.110000\n",
      "step 960,train cost=0.000544,acc=0.150000\n",
      "step 970,train cost=0.000556,acc=0.130000\n",
      "step 980,train cost=0.000560,acc=0.110000\n",
      "step 990,train cost=0.000556,acc=0.120000\n",
      "step 1000,train cost=0.000600,acc=0.080000\n",
      "step 1010,train cost=0.000591,acc=0.100000\n",
      "step 1020,train cost=0.000554,acc=0.180000\n",
      "step 1030,train cost=0.000547,acc=0.170000\n",
      "step 1040,train cost=0.000500,acc=0.200000\n",
      "step 1050,train cost=0.000524,acc=0.170000\n",
      "step 1060,train cost=0.000563,acc=0.090000\n",
      "step 1070,train cost=0.000566,acc=0.160000\n",
      "step 1080,train cost=0.000543,acc=0.120000\n",
      "step 1090,train cost=0.000513,acc=0.110000\n",
      "step 1100,train cost=0.000588,acc=0.080000\n",
      "step 1110,train cost=0.000547,acc=0.110000\n",
      "step 1120,train cost=0.000541,acc=0.180000\n",
      "step 1130,train cost=0.000491,acc=0.180000\n",
      "step 1140,train cost=0.000547,acc=0.130000\n",
      "step 1150,train cost=0.000542,acc=0.160000\n",
      "step 1160,train cost=0.000572,acc=0.100000\n",
      "step 1170,train cost=0.000577,acc=0.150000\n",
      "step 1180,train cost=0.000558,acc=0.160000\n",
      "step 1190,train cost=0.000572,acc=0.160000\n",
      "step 1200,train cost=0.000525,acc=0.110000\n",
      "step 1210,train cost=0.000521,acc=0.170000\n",
      "step 1220,train cost=0.000553,acc=0.130000\n",
      "step 1230,train cost=0.000509,acc=0.160000\n",
      "step 1240,train cost=0.000514,acc=0.170000\n",
      "step 1250,train cost=0.000565,acc=0.110000\n",
      "step 1260,train cost=0.000490,acc=0.160000\n",
      "step 1270,train cost=0.000548,acc=0.090000\n",
      "step 1280,train cost=0.000531,acc=0.150000\n",
      "step 1290,train cost=0.000488,acc=0.190000\n",
      "step 1300,train cost=0.000522,acc=0.110000\n",
      "step 1310,train cost=0.000493,acc=0.200000\n",
      "step 1320,train cost=0.000499,acc=0.180000\n",
      "step 1330,train cost=0.000498,acc=0.200000\n",
      "step 1340,train cost=0.000529,acc=0.160000\n",
      "step 1350,train cost=0.000535,acc=0.140000\n",
      "step 1360,train cost=0.000531,acc=0.150000\n",
      "step 1370,train cost=0.000514,acc=0.160000\n",
      "step 1380,train cost=0.000486,acc=0.150000\n",
      "step 1390,train cost=0.000508,acc=0.190000\n",
      "step 1400,train cost=0.000548,acc=0.150000\n",
      "step 1410,train cost=0.000542,acc=0.120000\n",
      "step 1420,train cost=0.000535,acc=0.160000\n",
      "step 1430,train cost=0.000522,acc=0.100000\n",
      "step 1440,train cost=0.000520,acc=0.130000\n",
      "step 1450,train cost=0.000521,acc=0.110000\n",
      "step 1460,train cost=0.000528,acc=0.140000\n",
      "step 1470,train cost=0.000534,acc=0.130000\n",
      "step 1480,train cost=0.000510,acc=0.120000\n",
      "step 1490,train cost=0.000513,acc=0.140000\n",
      "step 1500,train cost=0.000505,acc=0.140000\n",
      "step 1510,train cost=0.000530,acc=0.150000\n",
      "step 1520,train cost=0.000509,acc=0.130000\n",
      "step 1530,train cost=0.000536,acc=0.120000\n",
      "step 1540,train cost=0.000530,acc=0.070000\n",
      "step 1550,train cost=0.000489,acc=0.170000\n",
      "step 1560,train cost=0.000494,acc=0.190000\n",
      "step 1570,train cost=0.000460,acc=0.160000\n",
      "step 1580,train cost=0.000497,acc=0.160000\n",
      "step 1590,train cost=0.000502,acc=0.120000\n",
      "step 1600,train cost=0.000498,acc=0.160000\n",
      "step 1610,train cost=0.000524,acc=0.160000\n",
      "step 1620,train cost=0.000506,acc=0.130000\n",
      "step 1630,train cost=0.000536,acc=0.110000\n",
      "step 1640,train cost=0.000466,acc=0.170000\n",
      "step 1650,train cost=0.000495,acc=0.170000\n",
      "step 1660,train cost=0.000497,acc=0.160000\n",
      "step 1670,train cost=0.000477,acc=0.150000\n",
      "step 1680,train cost=0.000486,acc=0.150000\n",
      "step 1690,train cost=0.000483,acc=0.210000\n",
      "step 1700,train cost=0.000496,acc=0.180000\n",
      "step 1710,train cost=0.000512,acc=0.120000\n",
      "step 1720,train cost=0.000502,acc=0.160000\n",
      "step 1730,train cost=0.000446,acc=0.180000\n",
      "step 1740,train cost=0.000453,acc=0.180000\n",
      "step 1750,train cost=0.000509,acc=0.150000\n",
      "step 1760,train cost=0.000474,acc=0.230000\n",
      "step 1770,train cost=0.000464,acc=0.170000\n",
      "step 1780,train cost=0.000514,acc=0.170000\n",
      "step 1790,train cost=0.000476,acc=0.130000\n",
      "step 1800,train cost=0.000469,acc=0.160000\n",
      "step 1810,train cost=0.000472,acc=0.160000\n",
      "step 1820,train cost=0.000488,acc=0.150000\n",
      "step 1830,train cost=0.000500,acc=0.140000\n",
      "step 1840,train cost=0.000543,acc=0.080000\n",
      "step 1850,train cost=0.000501,acc=0.160000\n",
      "step 1860,train cost=0.000498,acc=0.110000\n",
      "step 1870,train cost=0.000475,acc=0.160000\n",
      "step 1880,train cost=0.000514,acc=0.160000\n",
      "step 1890,train cost=0.000454,acc=0.210000\n",
      "step 1900,train cost=0.000505,acc=0.150000\n",
      "step 1910,train cost=0.000475,acc=0.160000\n",
      "step 1920,train cost=0.000450,acc=0.130000\n",
      "step 1930,train cost=0.000442,acc=0.170000\n",
      "step 1940,train cost=0.000451,acc=0.220000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1950,train cost=0.000446,acc=0.210000\n",
      "step 1960,train cost=0.000433,acc=0.160000\n",
      "step 1970,train cost=0.000430,acc=0.190000\n",
      "step 1980,train cost=0.000470,acc=0.180000\n",
      "step 1990,train cost=0.000495,acc=0.120000\n",
      "step 2000,train cost=0.000432,acc=0.210000\n",
      "step 2010,train cost=0.000463,acc=0.180000\n",
      "step 2020,train cost=0.000416,acc=0.220000\n",
      "step 2030,train cost=0.000457,acc=0.150000\n",
      "step 2040,train cost=0.000455,acc=0.170000\n",
      "step 2050,train cost=0.000469,acc=0.140000\n",
      "step 2060,train cost=0.000445,acc=0.130000\n",
      "step 2070,train cost=0.000456,acc=0.200000\n",
      "step 2080,train cost=0.000465,acc=0.210000\n",
      "step 2090,train cost=0.000444,acc=0.250000\n",
      "step 2100,train cost=0.000449,acc=0.260000\n",
      "step 2110,train cost=0.000405,acc=0.220000\n",
      "step 2120,train cost=0.000410,acc=0.230000\n",
      "step 2130,train cost=0.000397,acc=0.190000\n",
      "step 2140,train cost=0.000396,acc=0.260000\n",
      "step 2150,train cost=0.000397,acc=0.230000\n",
      "step 2160,train cost=0.000408,acc=0.230000\n",
      "step 2170,train cost=0.000433,acc=0.190000\n",
      "step 2180,train cost=0.000405,acc=0.240000\n",
      "step 2190,train cost=0.000414,acc=0.210000\n",
      "step 2200,train cost=0.000406,acc=0.230000\n",
      "step 2210,train cost=0.000415,acc=0.280000\n",
      "step 2220,train cost=0.000354,acc=0.300000\n",
      "step 2230,train cost=0.000446,acc=0.240000\n",
      "step 2240,train cost=0.000434,acc=0.160000\n",
      "step 2250,train cost=0.000396,acc=0.200000\n",
      "step 2260,train cost=0.000394,acc=0.210000\n",
      "step 2270,train cost=0.000417,acc=0.260000\n",
      "step 2280,train cost=0.000426,acc=0.200000\n",
      "step 2290,train cost=0.000417,acc=0.160000\n",
      "step 2300,train cost=0.000419,acc=0.160000\n",
      "step 2310,train cost=0.000481,acc=0.130000\n",
      "step 2320,train cost=0.000440,acc=0.150000\n",
      "step 2330,train cost=0.000400,acc=0.230000\n",
      "step 2340,train cost=0.000397,acc=0.220000\n",
      "step 2350,train cost=0.000363,acc=0.300000\n",
      "step 2360,train cost=0.000408,acc=0.230000\n",
      "step 2370,train cost=0.000406,acc=0.250000\n",
      "step 2380,train cost=0.000394,acc=0.290000\n",
      "step 2390,train cost=0.000418,acc=0.210000\n",
      "step 2400,train cost=0.000426,acc=0.150000\n",
      "step 2410,train cost=0.000421,acc=0.210000\n",
      "step 2420,train cost=0.000403,acc=0.210000\n",
      "step 2430,train cost=0.000395,acc=0.260000\n",
      "step 2440,train cost=0.000384,acc=0.220000\n",
      "step 2450,train cost=0.000400,acc=0.210000\n",
      "step 2460,train cost=0.000398,acc=0.230000\n",
      "step 2470,train cost=0.000408,acc=0.250000\n",
      "step 2480,train cost=0.000351,acc=0.240000\n",
      "step 2490,train cost=0.000360,acc=0.270000\n",
      "step 2500,train cost=0.000379,acc=0.300000\n",
      "step 2510,train cost=0.000431,acc=0.210000\n",
      "step 2520,train cost=0.000400,acc=0.240000\n",
      "step 2530,train cost=0.000395,acc=0.210000\n",
      "step 2540,train cost=0.000393,acc=0.190000\n",
      "step 2550,train cost=0.000370,acc=0.250000\n",
      "step 2560,train cost=0.000380,acc=0.220000\n",
      "step 2570,train cost=0.000433,acc=0.160000\n",
      "step 2580,train cost=0.000340,acc=0.270000\n",
      "step 2590,train cost=0.000408,acc=0.180000\n",
      "step 2600,train cost=0.000370,acc=0.260000\n",
      "step 2610,train cost=0.000347,acc=0.280000\n",
      "step 2620,train cost=0.000384,acc=0.180000\n",
      "step 2630,train cost=0.000337,acc=0.320000\n",
      "step 2640,train cost=0.000374,acc=0.280000\n",
      "step 2650,train cost=0.000338,acc=0.310000\n",
      "step 2660,train cost=0.000327,acc=0.290000\n",
      "step 2670,train cost=0.000376,acc=0.260000\n",
      "step 2680,train cost=0.000387,acc=0.180000\n",
      "step 2690,train cost=0.000382,acc=0.240000\n",
      "step 2700,train cost=0.000355,acc=0.300000\n",
      "step 2710,train cost=0.000350,acc=0.290000\n",
      "step 2720,train cost=0.000336,acc=0.350000\n",
      "step 2730,train cost=0.000357,acc=0.210000\n",
      "step 2740,train cost=0.000378,acc=0.230000\n",
      "step 2750,train cost=0.000362,acc=0.250000\n",
      "step 2760,train cost=0.000366,acc=0.250000\n",
      "step 2770,train cost=0.000319,acc=0.310000\n",
      "step 2780,train cost=0.000307,acc=0.340000\n",
      "step 2790,train cost=0.000325,acc=0.320000\n",
      "step 2800,train cost=0.000336,acc=0.330000\n",
      "step 2810,train cost=0.000360,acc=0.280000\n",
      "step 2820,train cost=0.000326,acc=0.360000\n",
      "step 2830,train cost=0.000292,acc=0.390000\n",
      "step 2840,train cost=0.000303,acc=0.400000\n",
      "step 2850,train cost=0.000315,acc=0.280000\n",
      "step 2860,train cost=0.000320,acc=0.320000\n",
      "step 2870,train cost=0.000309,acc=0.350000\n",
      "step 2880,train cost=0.000327,acc=0.290000\n",
      "step 2890,train cost=0.000310,acc=0.320000\n",
      "step 2900,train cost=0.000323,acc=0.330000\n",
      "step 2910,train cost=0.000330,acc=0.300000\n",
      "step 2920,train cost=0.000278,acc=0.420000\n",
      "step 2930,train cost=0.000283,acc=0.420000\n",
      "step 2940,train cost=0.000299,acc=0.320000\n",
      "step 2950,train cost=0.000351,acc=0.280000\n",
      "step 2960,train cost=0.000298,acc=0.380000\n",
      "step 2970,train cost=0.000329,acc=0.310000\n",
      "step 2980,train cost=0.000318,acc=0.310000\n",
      "step 2990,train cost=0.000322,acc=0.310000\n",
      "step 3000,train cost=0.000341,acc=0.310000\n",
      "step 3010,train cost=0.000304,acc=0.310000\n",
      "step 3020,train cost=0.000301,acc=0.360000\n",
      "step 3030,train cost=0.000342,acc=0.320000\n",
      "step 3040,train cost=0.000283,acc=0.360000\n",
      "step 3050,train cost=0.000291,acc=0.380000\n",
      "step 3060,train cost=0.000299,acc=0.390000\n",
      "step 3070,train cost=0.000308,acc=0.310000\n",
      "step 3080,train cost=0.000333,acc=0.330000\n",
      "step 3090,train cost=0.000313,acc=0.280000\n",
      "step 3100,train cost=0.000297,acc=0.350000\n",
      "step 3110,train cost=0.000320,acc=0.270000\n",
      "step 3120,train cost=0.000262,acc=0.460000\n",
      "step 3130,train cost=0.000300,acc=0.420000\n",
      "step 3140,train cost=0.000300,acc=0.330000\n",
      "step 3150,train cost=0.000255,acc=0.440000\n",
      "step 3160,train cost=0.000279,acc=0.390000\n",
      "step 3170,train cost=0.000299,acc=0.340000\n",
      "step 3180,train cost=0.000306,acc=0.310000\n",
      "step 3190,train cost=0.000285,acc=0.420000\n",
      "step 3200,train cost=0.000254,acc=0.480000\n",
      "step 3210,train cost=0.000283,acc=0.400000\n",
      "step 3220,train cost=0.000297,acc=0.350000\n",
      "step 3230,train cost=0.000265,acc=0.400000\n",
      "step 3240,train cost=0.000300,acc=0.350000\n",
      "step 3250,train cost=0.000269,acc=0.310000\n",
      "step 3260,train cost=0.000289,acc=0.350000\n",
      "step 3270,train cost=0.000284,acc=0.410000\n",
      "step 3280,train cost=0.000257,acc=0.460000\n",
      "step 3290,train cost=0.000266,acc=0.460000\n",
      "step 3300,train cost=0.000262,acc=0.440000\n",
      "step 3310,train cost=0.000295,acc=0.350000\n",
      "step 3320,train cost=0.000223,acc=0.490000\n",
      "step 3330,train cost=0.000295,acc=0.390000\n",
      "step 3340,train cost=0.000309,acc=0.350000\n",
      "step 3350,train cost=0.000279,acc=0.400000\n",
      "step 3360,train cost=0.000272,acc=0.370000\n",
      "step 3370,train cost=0.000224,acc=0.460000\n",
      "step 3380,train cost=0.000260,acc=0.440000\n",
      "step 3390,train cost=0.000245,acc=0.470000\n",
      "step 3400,train cost=0.000249,acc=0.390000\n",
      "step 3410,train cost=0.000261,acc=0.480000\n",
      "step 3420,train cost=0.000268,acc=0.400000\n",
      "step 3430,train cost=0.000251,acc=0.440000\n",
      "step 3440,train cost=0.000254,acc=0.420000\n",
      "step 3450,train cost=0.000231,acc=0.510000\n",
      "step 3460,train cost=0.000233,acc=0.450000\n",
      "step 3470,train cost=0.000244,acc=0.440000\n",
      "step 3480,train cost=0.000256,acc=0.400000\n",
      "step 3490,train cost=0.000243,acc=0.410000\n",
      "step 3500,train cost=0.000207,acc=0.520000\n",
      "step 3510,train cost=0.000225,acc=0.520000\n",
      "step 3520,train cost=0.000246,acc=0.450000\n",
      "step 3530,train cost=0.000269,acc=0.450000\n",
      "step 3540,train cost=0.000224,acc=0.510000\n",
      "step 3550,train cost=0.000239,acc=0.420000\n",
      "step 3560,train cost=0.000202,acc=0.540000\n",
      "step 3570,train cost=0.000215,acc=0.470000\n",
      "step 3580,train cost=0.000250,acc=0.470000\n",
      "step 3590,train cost=0.000256,acc=0.370000\n",
      "step 3600,train cost=0.000218,acc=0.520000\n",
      "step 3610,train cost=0.000242,acc=0.460000\n",
      "step 3620,train cost=0.000220,acc=0.550000\n",
      "step 3630,train cost=0.000217,acc=0.470000\n",
      "step 3640,train cost=0.000192,acc=0.510000\n",
      "step 3650,train cost=0.000210,acc=0.470000\n",
      "step 3660,train cost=0.000199,acc=0.500000\n",
      "step 3670,train cost=0.000231,acc=0.440000\n",
      "step 3680,train cost=0.000207,acc=0.520000\n",
      "step 3690,train cost=0.000208,acc=0.500000\n",
      "step 3700,train cost=0.000213,acc=0.540000\n",
      "step 3710,train cost=0.000226,acc=0.440000\n",
      "step 3720,train cost=0.000187,acc=0.540000\n",
      "step 3730,train cost=0.000190,acc=0.550000\n",
      "step 3740,train cost=0.000192,acc=0.540000\n",
      "step 3750,train cost=0.000200,acc=0.530000\n",
      "step 3760,train cost=0.000235,acc=0.460000\n",
      "step 3770,train cost=0.000202,acc=0.530000\n",
      "step 3780,train cost=0.000208,acc=0.560000\n",
      "step 3790,train cost=0.000186,acc=0.530000\n",
      "step 3800,train cost=0.000204,acc=0.530000\n",
      "step 3810,train cost=0.000212,acc=0.470000\n",
      "step 3820,train cost=0.000228,acc=0.480000\n",
      "step 3830,train cost=0.000185,acc=0.550000\n",
      "step 3840,train cost=0.000185,acc=0.600000\n",
      "step 3850,train cost=0.000176,acc=0.600000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3860,train cost=0.000185,acc=0.570000\n",
      "step 3870,train cost=0.000208,acc=0.520000\n",
      "step 3880,train cost=0.000196,acc=0.540000\n",
      "step 3890,train cost=0.000182,acc=0.520000\n",
      "step 3900,train cost=0.000151,acc=0.660000\n",
      "step 3910,train cost=0.000166,acc=0.660000\n",
      "step 3920,train cost=0.000183,acc=0.520000\n",
      "step 3930,train cost=0.000167,acc=0.640000\n",
      "step 3940,train cost=0.000196,acc=0.530000\n",
      "step 3950,train cost=0.000154,acc=0.600000\n",
      "step 3960,train cost=0.000170,acc=0.610000\n",
      "step 3970,train cost=0.000174,acc=0.650000\n",
      "step 3980,train cost=0.000178,acc=0.580000\n",
      "step 3990,train cost=0.000185,acc=0.570000\n",
      "step 4000,train cost=0.000224,acc=0.520000\n",
      "step 4010,train cost=0.000183,acc=0.540000\n",
      "step 4020,train cost=0.000143,acc=0.630000\n",
      "step 4030,train cost=0.000166,acc=0.600000\n",
      "step 4040,train cost=0.000184,acc=0.530000\n",
      "step 4050,train cost=0.000146,acc=0.620000\n",
      "step 4060,train cost=0.000170,acc=0.590000\n",
      "step 4070,train cost=0.000188,acc=0.570000\n",
      "step 4080,train cost=0.000156,acc=0.640000\n",
      "step 4090,train cost=0.000165,acc=0.570000\n",
      "step 4100,train cost=0.000178,acc=0.530000\n",
      "step 4110,train cost=0.000135,acc=0.640000\n",
      "step 4120,train cost=0.000179,acc=0.490000\n",
      "step 4130,train cost=0.000169,acc=0.540000\n",
      "step 4140,train cost=0.000138,acc=0.700000\n",
      "step 4150,train cost=0.000178,acc=0.560000\n",
      "step 4160,train cost=0.000167,acc=0.520000\n",
      "step 4170,train cost=0.000168,acc=0.530000\n",
      "step 4180,train cost=0.000152,acc=0.660000\n",
      "step 4190,train cost=0.000192,acc=0.590000\n",
      "step 4200,train cost=0.000176,acc=0.620000\n",
      "step 4210,train cost=0.000131,acc=0.670000\n",
      "step 4220,train cost=0.000147,acc=0.620000\n",
      "step 4230,train cost=0.000152,acc=0.680000\n",
      "step 4240,train cost=0.000140,acc=0.630000\n",
      "step 4250,train cost=0.000136,acc=0.690000\n",
      "step 4260,train cost=0.000138,acc=0.680000\n",
      "step 4270,train cost=0.000147,acc=0.620000\n",
      "step 4280,train cost=0.000113,acc=0.700000\n",
      "step 4290,train cost=0.000172,acc=0.540000\n",
      "step 4300,train cost=0.000158,acc=0.590000\n",
      "step 4310,train cost=0.000151,acc=0.620000\n",
      "step 4320,train cost=0.000140,acc=0.640000\n",
      "step 4330,train cost=0.000163,acc=0.630000\n",
      "step 4340,train cost=0.000147,acc=0.570000\n",
      "step 4350,train cost=0.000160,acc=0.600000\n",
      "step 4360,train cost=0.000136,acc=0.710000\n",
      "step 4370,train cost=0.000138,acc=0.690000\n",
      "step 4380,train cost=0.000155,acc=0.600000\n",
      "step 4390,train cost=0.000138,acc=0.680000\n",
      "step 4400,train cost=0.000159,acc=0.620000\n",
      "step 4410,train cost=0.000134,acc=0.680000\n",
      "step 4420,train cost=0.000154,acc=0.620000\n",
      "step 4430,train cost=0.000115,acc=0.650000\n",
      "step 4440,train cost=0.000111,acc=0.750000\n",
      "step 4450,train cost=0.000139,acc=0.660000\n",
      "step 4460,train cost=0.000154,acc=0.630000\n",
      "step 4470,train cost=0.000148,acc=0.630000\n",
      "step 4480,train cost=0.000125,acc=0.680000\n",
      "step 4490,train cost=0.000138,acc=0.710000\n",
      "step 4500,train cost=0.000136,acc=0.650000\n",
      "step 4510,train cost=0.000140,acc=0.650000\n",
      "step 4520,train cost=0.000129,acc=0.660000\n",
      "step 4530,train cost=0.000118,acc=0.720000\n",
      "step 4540,train cost=0.000122,acc=0.680000\n",
      "step 4550,train cost=0.000166,acc=0.610000\n",
      "step 4560,train cost=0.000134,acc=0.710000\n",
      "step 4570,train cost=0.000116,acc=0.750000\n",
      "step 4580,train cost=0.000124,acc=0.730000\n",
      "step 4590,train cost=0.000117,acc=0.710000\n",
      "step 4600,train cost=0.000113,acc=0.720000\n",
      "step 4610,train cost=0.000138,acc=0.690000\n",
      "step 4620,train cost=0.000150,acc=0.610000\n",
      "step 4630,train cost=0.000125,acc=0.720000\n",
      "step 4640,train cost=0.000117,acc=0.720000\n",
      "step 4650,train cost=0.000127,acc=0.670000\n",
      "step 4660,train cost=0.000136,acc=0.660000\n",
      "step 4670,train cost=0.000114,acc=0.760000\n",
      "step 4680,train cost=0.000131,acc=0.700000\n",
      "step 4690,train cost=0.000111,acc=0.740000\n",
      "step 4700,train cost=0.000121,acc=0.690000\n",
      "step 4710,train cost=0.000110,acc=0.790000\n",
      "step 4720,train cost=0.000108,acc=0.730000\n",
      "step 4730,train cost=0.000110,acc=0.730000\n",
      "step 4740,train cost=0.000126,acc=0.740000\n",
      "step 4750,train cost=0.000119,acc=0.690000\n",
      "step 4760,train cost=0.000115,acc=0.670000\n",
      "step 4770,train cost=0.000111,acc=0.720000\n",
      "step 4780,train cost=0.000112,acc=0.740000\n",
      "step 4790,train cost=0.000125,acc=0.670000\n",
      "step 4800,train cost=0.000102,acc=0.760000\n",
      "step 4810,train cost=0.000126,acc=0.710000\n",
      "step 4820,train cost=0.000110,acc=0.680000\n",
      "step 4830,train cost=0.000110,acc=0.730000\n",
      "step 4840,train cost=0.000107,acc=0.730000\n",
      "step 4850,train cost=0.000126,acc=0.700000\n",
      "step 4860,train cost=0.000104,acc=0.770000\n",
      "step 4870,train cost=0.000089,acc=0.790000\n",
      "step 4880,train cost=0.000143,acc=0.680000\n",
      "step 4890,train cost=0.000102,acc=0.780000\n",
      "step 4900,train cost=0.000089,acc=0.770000\n",
      "step 4910,train cost=0.000098,acc=0.770000\n",
      "step 4920,train cost=0.000109,acc=0.690000\n",
      "step 4930,train cost=0.000083,acc=0.790000\n",
      "step 4940,train cost=0.000113,acc=0.770000\n",
      "step 4950,train cost=0.000094,acc=0.770000\n",
      "step 4960,train cost=0.000090,acc=0.780000\n",
      "step 4970,train cost=0.000122,acc=0.660000\n",
      "step 4980,train cost=0.000097,acc=0.780000\n",
      "step 4990,train cost=0.000081,acc=0.830000\n",
      "step 5000,train cost=0.000109,acc=0.730000\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    _batch_size=100\n",
    "    X_batch, y_batch = get_next_batch(_batch_size)\n",
    "    cost, acc, _ = sess.run([loss,accuracy,train_op],feed_dict={X: X_batch, y_input:y_batch,keep_prob:0.5, batch_size: _batch_size})\n",
    "    if (i+1)%10 == 0:\n",
    "        print(\"step {},train cost={:.6f},acc={:.6f}\".format(i+1,cost,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_index(vector):\n",
    "    threshold = np.random.rand(1)\n",
    "    #print(\"threshold is:\",threshold)\n",
    "    i = 0\n",
    "    proba_sum = 0.0\n",
    "    while(i<len(vector)):\n",
    "        proba_sum += vector[i]\n",
    "        if(proba_sum>threshold):\n",
    "            #print(\"Find i:\",i)\n",
    "            break\n",
    "        i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fake_Old_Baggins(len_to_write):\n",
    "    #text = [\"gandalf\",\"was\",\"reading\",\"a\",\"riddle\",\"written\",\"in\"]\n",
    "    #kickoff_phrase = [\"gandalf\",\"was\",\"reading\",\"a\",\"riddle\",\"written\",\"in\"]\n",
    "    text = [\"gollum\",\"was\",\"catching\",\"fish\",\"by\",\"the\",\"lake\"]\n",
    "    kickoff_phrase = [\"gollum\",\"was\",\"catching\",\"fish\",\"by\",\"the\",\"lake\"]\n",
    "    i = 0\n",
    "    while(i < len_to_write):\n",
    "        kickoff_vector = []\n",
    "        for j in range(timestep_size):\n",
    "            kickoff_vector.append(w2v.wv.get_vector(kickoff_phrase[j]))\n",
    "        kickoff_batch = []\n",
    "        kickoff_batch.append(kickoff_vector)\n",
    "        kickoff_batch = np.array(kickoff_batch)\n",
    "        \n",
    "        predict_vector = sess.run([y_pre],feed_dict={X: kickoff_batch, keep_prob:1.0,batch_size: 1})\n",
    "        predict_vector = np.array(predict_vector)\n",
    "        predict_word = label_encoder.inverse_transform(get_next_index(predict_vector[0][0]))\n",
    "        \n",
    "        kickoff_phrase.pop(0)\n",
    "        kickoff_phrase.append(predict_word)\n",
    "        text.append(predict_word)\n",
    "        \n",
    "        #print(kickoff_phrase)\n",
    "        \n",
    "        i += 1\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gandalf was reading a riddle written in most ; and they dwelt taller became scarlet , and they could do . at time they had got to wish it . they were very of fact , sheltered , but thorin and well-ordered ( the speech ) , who had always nothing new among the different in elrond ; but when frodo had not the marriage of him ; and he was getting restless of light . he looked at the door on his eyes . no apparent , for a present , not he meant . ' 'it has come far to give such about this ring\n"
     ]
    }
   ],
   "source": [
    "rslt = Fake_Old_Baggins(100)\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gandalf was reading a riddle written in the black of the shire . 'so to the shire-holidays of the stories of the thain . bilbo was elected after a table in the black . will not no are the , or of the long . if it was his ring , i say , and he made ever for their own ; and both it , and go , his nuisance . ' 'then 'that '' . ' well i do to know ' this letters , and i could not an . it seemed a great selection creature . ' 'and i 've n't oiolosslo my own , anyway you , i saw no new . i have said good-bye . but i have n't say to him his mind the wretched to become . he becomes to go alone , and the other of those mountains must be roots over from the bag edge at the outlandish . the bounders of the shire , and no point , and that was not much or doubt . there was no pleasant of a legend of old m pass the party , though i still you not say what that this ai thought no elm of the ring knew in its fourth , with the air book . before the last battle on the wild power that to it himself . he said the bag . he spoke down the wall on a glowing like an in the fire , and they turned to fetch at the same . just the your was consumed . the addition baggins has the most the mayor . i have all how that have resist ages strong and admirable . 'and a , of course , these one of the world would not him . this that everything spoke a few\n"
     ]
    }
   ],
   "source": [
    "rslt2 = Fake_Old_Baggins(300)\n",
    "print(rslt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt3 = Fake_Old_Baggins(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gollum was catching fish by the lake pipe-weed son of the shire centuries kind first days of the shire , it was before the true for that he had nothing , of course , and very were their heads . the others qualities , and he s to be sure of him , ' said gandalf . 'and he kept that his will to be very with bilbo and he retired the good in him , and the old of building odd ; but there is in a great of the power , stern and his engrossing branch were peregrin and disliked them . and they were maker through the study , and the stride and knives -paper and the reminiscences , and some reed-beds and were the handling ( started as had said ) . the ring would not make only it . i do not i shall that has come or see by you . i deserve , perhaps , like hive - not expression . he got got the knowledge , till he could an , and the thought held to say . and elf-towers flash , as an present , and this 's mr. 's party . i love to get . i 'm not trying it came in a new . but the finally folk the true account them ; but many farms of them , and made the other that they had never become of the king . but he was lost in the bree and in the shire of the shire , whom they refer to as `` colonists '' ; but gollum was still bright , deep , as i hope , and after soon as i can not know yet like . bilbo 's place . so the first he had overcome little busy and drink than the\n"
     ]
    }
   ],
   "source": [
    "print(rslt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gollum was catching fish by the lake ; rose long body , before the fireworks of the wise , and in the language of those lands the elves of these , and they were accurate in whispers . they contained in the mountains , and often all it was , . being had been done ; and i like he often old , and it was staying in it . he seemed very point hoots with bilbo 's inhabitants . from there he gave to the sun , menacing with his hand to turned . 'i they ? ' go , ' said the wizard . 'you have never been the true , and he takes it , and he got up and drained on his own . in apparent end locked the long was long divided : the shadow of the crowd and blocked . the mathom-house were broader of the they . of the westlands of eriador , between the misty mountains and the mountains mountains , and far soon were as skilful , sheltered , their own and long curious . they were hospitable marked in treasure , and the crumbs strongholds cracker the game in disappearing . the old of building farmhouses ,\n"
     ]
    }
   ],
   "source": [
    "print(rslt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_group = 8\n",
    "\n",
    "random_range = int(len(words)/timestep_group) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(batch_size):\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    index = np.random.randint(0,random_range,batch_size)\n",
    "    #print(\"index: \",index)\n",
    "    for i in range(batch_size):\n",
    "        temp = []\n",
    "        for j in range(timestep_group-1):\n",
    "            #print(j,\"Get word:\",words[index[i]+j])\n",
    "            temp.append(w2v.wv.get_vector(words[index[i]+j]))\n",
    "        X_batch.append(temp)\n",
    "        #print(\"Label is:\", words[index[i]+timestep_group-1])\n",
    "        Y_batch.append(w2v.wv.get_vector(words[index[i]+timestep_group-1]))\n",
    "    return np.array(X_batch),np.array(Y_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0005\n",
    "input_size = emb_dim\n",
    "timestep_size = timestep_group - 1\n",
    "hidden_size = 512\n",
    "layer_num = 2 \n",
    "cell_type = \"lstm\"\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 7, emb_dim])\n",
    "y_input = tf.placeholder(tf.float32, [None, emb_dim])\n",
    "batch_size = tf.placeholder(tf.int32, [])\n",
    "keep_prob = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(cell_type,num_nodes,keep_prob):\n",
    "    # assert(cell_type in [\"lstm\",\"block_lstm\"],\"Wrong cell type\")\n",
    "    if cell_type == \"lstm\":\n",
    "        cell = rnn.BasicLSTMCell(num_nodes)\n",
    "    else:\n",
    "        cell = rnn.LSTMBlockCell(num_nodes)\n",
    "    cell = rnn.DropoutWrapper(cell,output_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "mlstm_cell = rnn.MultiRNNCell([lstm_cell(cell_type,hidden_size,keep_prob) for _ in range(layer_num)],state_is_tuple=True)\n",
    "init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = list()\n",
    "state = init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('RNN'):\n",
    "    for timestep in range(timestep_size):\n",
    "        (cell_output,state) = mlstm_cell(X[:,timestep,:],state)\n",
    "        outputs.append(cell_output)\n",
    "h_state = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([hidden_size,emb_dim],stddev=0.1),dtype=tf.float32)\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[emb_dim]),dtype=tf.float32)\n",
    "y_pre = tf.matmul(h_state,W) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y_input,y_pre)\n",
    "train_op = tf.train.AdamOptimizer(alpha).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time0 = time.time()\n",
    "for i in range(5000):\n",
    "    _batch_size=100\n",
    "    X_batch, y_batch = get_next_batch(_batch_size)\n",
    "    cost, _ = sess.run([loss,train_op],feed_dict={X: X_batch, y_input:y_batch,keep_prob:0.5, batch_size: _batch_size})\n",
    "    if (i+1)%10 == 0:\n",
    "        print(\"step {},train cost={:.6f}\".format(i+1,cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kickoff_phrase = [\"Gandalf\",\"is\",\"reading\",\"a\",\"riddle\",\"on\",\"a\"]\n",
    "kickoff_phrase = [\"is\",\"reading\",\"a\",\"riddle\",\"on\",\"a\",\"track\"]\n",
    "kickoff_vector = []\n",
    "for i in range(timestep_size):\n",
    "    kickoff_vector.append(w2v.wv.get_vector(kickoff_phrase[i]))\n",
    "kickoff_batch = []\n",
    "kickoff_batch.append(kickoff_vector)\n",
    "kickoff_batch = np.array(kickoff_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 400)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kickoff_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vector = sess.run([y_pre],feed_dict={X: kickoff_batch, keep_prob:1.0,batch_size: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
