{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/py3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986848"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data, make all to lowercase\n",
    "txt = open(\"theLordOfTheRings.txt\")\n",
    "data = txt.read()\n",
    "data = data.lower()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the lord of the rings 1. concerning hobbits this book is largely concerned with hobbits, and from its pages a reader may discover much of their character and a little of their history.  further inform'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the lord of the rings 1. concerning hobbits this book is largely concerned with hobbits, and from its pages a reader may discover much of their character and a little of their history.',\n",
       " 'further information will also be found in the selection from the red book of westmarch that has already been published, under the title of the hobbit.',\n",
       " 'that story was derived from the earlier chapters of the red book, composed by bilbo himself, the first hobbit to become famous in the world at large, and called by him there and back again, since they told of his journey into the east and his return: an adventure which later involved all the hobbits in the great events of that age that are here related.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(data)\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218302"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(data)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['further',\n",
       "  'information',\n",
       "  'will',\n",
       "  'also',\n",
       "  'be',\n",
       "  'found',\n",
       "  'in',\n",
       "  'the',\n",
       "  'selection',\n",
       "  'from',\n",
       "  'the',\n",
       "  'red',\n",
       "  'book',\n",
       "  'of',\n",
       "  'westmarch',\n",
       "  'that',\n",
       "  'has',\n",
       "  'already',\n",
       "  'been',\n",
       "  'published',\n",
       "  ',',\n",
       "  'under',\n",
       "  'the',\n",
       "  'title',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hobbit',\n",
       "  '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_break_down = [word_tokenize(sentence) for sentence in sentences ]\n",
    "len(sentences_break_down)\n",
    "sentences_break_down[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Word2Vec to convert word to vector, length 400\n",
    "w2v = Word2Vec(sentences=sentences_break_down, \n",
    "        sg=1,\n",
    "        size=emb_dim,\n",
    "        window=5,\n",
    "        alpha=0.0005,\n",
    "        min_count=1,\n",
    "        workers=8,\n",
    "        batch_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37466330, 54575500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_len = len(w2v.wv.vocab)\n",
    "w2v.train(sentences=sentences_break_down, total_words=w2v_len, epochs=250, start_alpha=0.0005, end_alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gandalf', 0.9999998807907104),\n",
       " ('strider', 0.9717492461204529),\n",
       " ('elrond', 0.9500234127044678),\n",
       " ('aragorn', 0.945915937423706),\n",
       " ('boromir', 0.9434243440628052),\n",
       " ('legolas', 0.930779218673706),\n",
       " ('haldir', 0.9294370412826538),\n",
       " ('frodo', 0.9274029731750488),\n",
       " ('gildor', 0.9201619625091553),\n",
       " ('pippin', 0.9197915196418762)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if the work is well done\n",
    "w2v.wv.similar_by_vector(w2v.wv.get_vector('gandalf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heart', 0.8911802768707275),\n",
       " ('gollum', 0.8909974098205566),\n",
       " ('thing', 0.8861624002456665),\n",
       " ('account', 0.8858187794685364),\n",
       " ('desire', 0.883201539516449),\n",
       " ('sauron', 0.8829586505889893),\n",
       " ('party', 0.8822845220565796),\n",
       " ('tale', 0.8818395733833313),\n",
       " ('mind', 0.8812507390975952),\n",
       " ('put', 0.8806129693984985)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive='ring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_dict = w2v.wv.vocab\n",
    "type(w2v_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dict for one code encoding \n",
    "w2v_keys = np.array(list(w2v_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'lord', 'of', 'rings', '1.', 'concerning', 'hobbits', 'this',\n",
       "       'book', 'is'],\n",
       "      dtype='<U22')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_keys[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8555 5287 6028 7081  529 2022 4474 8606 1382 4806]\n"
     ]
    }
   ],
   "source": [
    "# Step intermedia, do label encoding first \n",
    "label_encoder = LabelEncoder()\n",
    "w2v_keys_encoded = label_encoder.fit_transform(w2v_keys)\n",
    "print(w2v_keys_encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Get the mapping \n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "w2v_keys_encoded = w2v_keys_encoded.reshape(len(w2v_keys_encoded),1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(w2v_keys_encoded)\n",
    "print(onehot_encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted = label_encoder.inverse_transform([np.argmax(onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare groups of eight word to feed the RNN\n",
    "timestep_group = 8\n",
    "random_range = int(len(words)/timestep_group) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return batch_num groups randomly, come in handy soon \n",
    "def get_next_batch(batch_size):\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    index = np.random.randint(0,random_range,batch_size)\n",
    "    #print(\"index: \",index)\n",
    "    for i in range(batch_size):\n",
    "        temp = []\n",
    "        for j in range(timestep_group-1):\n",
    "            #print(j,\"Get word:\",words[index[i]+j])\n",
    "            temp.append(w2v.wv.get_vector(words[index[i]+j]))\n",
    "        X_batch.append(temp)\n",
    "        #print(\"Label is:\", words[index[i]+timestep_group-1])\n",
    "        temp = label_encoder.transform([words[index[i]+timestep_group-1]])\n",
    "        tempv = onehot_encoder.transform(temp.reshape(1,1))\n",
    "        Y_batch.append(tempv[0])\n",
    "    return np.array(X_batch),np.array(Y_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3                          # learning rate\n",
    "input_size = emb_dim                  # input size \n",
    "timestep_size = timestep_group - 1    # cell number in each layer \n",
    "hidden_size = 1024                    # one dimension for parameter matrix \n",
    "layer_num = 2                         # number of layer \n",
    "class_num = w2v_len                   # output size \n",
    "cell_type = \"lstm\"                  \n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, timestep_size, emb_dim])\n",
    "y_input = tf.placeholder(tf.float32, [None, class_num])\n",
    "batch_size = tf.placeholder(tf.int32, [])\n",
    "keep_prob = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the lstm cell\n",
    "def lstm_cell(cell_type,num_nodes,keep_prob):\n",
    "    assert(cell_type in [\"lstm\",\"block_lstm\"],\"Wrong cell type\")\n",
    "    if cell_type == \"lstm\":\n",
    "        cell = rnn.BasicLSTMCell(num_nodes)\n",
    "    else:\n",
    "        cell = rnn.LSTMBlockCell(num_nodes)\n",
    "    cell = rnn.DropoutWrapper(cell,output_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "mlstm_cell = rnn.MultiRNNCell([lstm_cell(cell_type,hidden_size,keep_prob) for _ in range(layer_num)],state_is_tuple=True)\n",
    "init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the final output of the RNN layers\n",
    "outputs = list()\n",
    "state = init_state\n",
    "\n",
    "with tf.variable_scope('RNN'):\n",
    "    for timestep in range(timestep_size):\n",
    "        (cell_output,state) = mlstm_cell(X[:,timestep,:],state)\n",
    "        outputs.append(cell_output)\n",
    "h_state = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Layer, y^ = hW + b \n",
    "W = tf.Variable(tf.truncated_normal([hidden_size,class_num],stddev=0.1),dtype=tf.float32)\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[class_num]),dtype=tf.float32)\n",
    "y_pre = tf.nn.softmax(tf.matmul(h_state,W) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize lost function \n",
    "loss = - tf.reduce_mean(y_input * tf.log(y_pre))\n",
    "train_op = tf.train.AdamOptimizer(alpha).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for display use during training\n",
    "correct_pred = tf.equal(tf.argmax(y_pre,1),tf.argmax(y_input,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10,train cost=0.000850,acc=0.040000\n",
      "step 20,train cost=0.000749,acc=0.040000\n",
      "step 30,train cost=0.000765,acc=0.000000\n",
      "step 40,train cost=0.000759,acc=0.010000\n",
      "step 50,train cost=0.000717,acc=0.050000\n",
      "step 60,train cost=0.000680,acc=0.080000\n",
      "step 70,train cost=0.000648,acc=0.090000\n",
      "step 80,train cost=0.000622,acc=0.050000\n",
      "step 90,train cost=0.000703,acc=0.030000\n",
      "step 100,train cost=0.000658,acc=0.060000\n",
      "step 110,train cost=0.000671,acc=0.050000\n",
      "step 120,train cost=0.000705,acc=0.080000\n",
      "step 130,train cost=0.000651,acc=0.070000\n",
      "step 140,train cost=0.000665,acc=0.030000\n",
      "step 150,train cost=0.000683,acc=0.030000\n",
      "step 160,train cost=0.000681,acc=0.080000\n",
      "step 170,train cost=0.000713,acc=0.050000\n",
      "step 180,train cost=0.000669,acc=0.080000\n",
      "step 190,train cost=0.000648,acc=0.070000\n",
      "step 200,train cost=0.000644,acc=0.070000\n",
      "step 210,train cost=0.000646,acc=0.060000\n",
      "step 220,train cost=0.000623,acc=0.090000\n",
      "step 230,train cost=0.000647,acc=0.070000\n",
      "step 240,train cost=0.000637,acc=0.090000\n",
      "step 250,train cost=0.000617,acc=0.050000\n",
      "step 260,train cost=0.000646,acc=0.080000\n",
      "step 270,train cost=0.000631,acc=0.060000\n",
      "step 280,train cost=0.000657,acc=0.020000\n",
      "step 290,train cost=0.000670,acc=0.080000\n",
      "step 300,train cost=0.000670,acc=0.060000\n",
      "step 310,train cost=0.000643,acc=0.050000\n",
      "step 320,train cost=0.000665,acc=0.050000\n",
      "step 330,train cost=0.000594,acc=0.030000\n",
      "step 340,train cost=0.000652,acc=0.030000\n",
      "step 350,train cost=0.000591,acc=0.070000\n",
      "step 360,train cost=0.000582,acc=0.070000\n",
      "step 370,train cost=0.000650,acc=0.050000\n",
      "step 380,train cost=0.000639,acc=0.040000\n",
      "step 390,train cost=0.000624,acc=0.110000\n",
      "step 400,train cost=0.000639,acc=0.060000\n",
      "step 410,train cost=0.000628,acc=0.050000\n",
      "step 420,train cost=0.000646,acc=0.050000\n",
      "step 430,train cost=0.000635,acc=0.100000\n",
      "step 440,train cost=0.000645,acc=0.100000\n",
      "step 450,train cost=0.000663,acc=0.090000\n",
      "step 460,train cost=0.000590,acc=0.090000\n",
      "step 470,train cost=0.000597,acc=0.090000\n",
      "step 480,train cost=0.000626,acc=0.110000\n",
      "step 490,train cost=0.000687,acc=0.060000\n",
      "step 500,train cost=0.000642,acc=0.080000\n",
      "step 510,train cost=0.000566,acc=0.110000\n",
      "step 520,train cost=0.000629,acc=0.060000\n",
      "step 530,train cost=0.000623,acc=0.080000\n",
      "step 540,train cost=0.000640,acc=0.070000\n",
      "step 550,train cost=0.000567,acc=0.080000\n",
      "step 560,train cost=0.000625,acc=0.100000\n",
      "step 570,train cost=0.000637,acc=0.090000\n",
      "step 580,train cost=0.000639,acc=0.090000\n",
      "step 590,train cost=0.000616,acc=0.090000\n",
      "step 600,train cost=0.000583,acc=0.120000\n",
      "step 610,train cost=0.000595,acc=0.110000\n",
      "step 620,train cost=0.000642,acc=0.050000\n",
      "step 630,train cost=0.000615,acc=0.110000\n",
      "step 640,train cost=0.000623,acc=0.080000\n",
      "step 650,train cost=0.000540,acc=0.150000\n",
      "step 660,train cost=0.000591,acc=0.080000\n",
      "step 670,train cost=0.000601,acc=0.100000\n",
      "step 680,train cost=0.000568,acc=0.110000\n",
      "step 690,train cost=0.000559,acc=0.160000\n",
      "step 700,train cost=0.000529,acc=0.120000\n",
      "step 710,train cost=0.000565,acc=0.140000\n",
      "step 720,train cost=0.000596,acc=0.110000\n",
      "step 730,train cost=0.000593,acc=0.100000\n",
      "step 740,train cost=0.000597,acc=0.100000\n",
      "step 750,train cost=0.000551,acc=0.130000\n",
      "step 760,train cost=0.000626,acc=0.090000\n",
      "step 770,train cost=0.000591,acc=0.110000\n",
      "step 780,train cost=0.000571,acc=0.130000\n",
      "step 790,train cost=0.000624,acc=0.130000\n",
      "step 800,train cost=0.000556,acc=0.130000\n",
      "step 810,train cost=0.000571,acc=0.170000\n",
      "step 820,train cost=0.000551,acc=0.150000\n",
      "step 830,train cost=0.000531,acc=0.140000\n",
      "step 840,train cost=0.000567,acc=0.140000\n",
      "step 850,train cost=0.000600,acc=0.080000\n",
      "step 860,train cost=0.000574,acc=0.080000\n",
      "step 870,train cost=0.000580,acc=0.070000\n",
      "step 880,train cost=0.000606,acc=0.100000\n",
      "step 890,train cost=0.000636,acc=0.060000\n",
      "step 900,train cost=0.000554,acc=0.120000\n",
      "step 910,train cost=0.000576,acc=0.130000\n",
      "step 920,train cost=0.000577,acc=0.120000\n",
      "step 930,train cost=0.000557,acc=0.110000\n",
      "step 940,train cost=0.000544,acc=0.200000\n",
      "step 950,train cost=0.000560,acc=0.130000\n",
      "step 960,train cost=0.000585,acc=0.120000\n",
      "step 970,train cost=0.000586,acc=0.090000\n",
      "step 980,train cost=0.000529,acc=0.120000\n",
      "step 990,train cost=0.000575,acc=0.080000\n",
      "step 1000,train cost=0.000570,acc=0.080000\n",
      "step 1010,train cost=0.000510,acc=0.120000\n",
      "step 1020,train cost=0.000556,acc=0.110000\n",
      "step 1030,train cost=0.000499,acc=0.220000\n",
      "step 1040,train cost=0.000520,acc=0.120000\n",
      "step 1050,train cost=0.000578,acc=0.160000\n",
      "step 1060,train cost=0.000526,acc=0.120000\n",
      "step 1070,train cost=0.000590,acc=0.100000\n",
      "step 1080,train cost=0.000572,acc=0.110000\n",
      "step 1090,train cost=0.000589,acc=0.110000\n",
      "step 1100,train cost=0.000523,acc=0.150000\n",
      "step 1110,train cost=0.000538,acc=0.120000\n",
      "step 1120,train cost=0.000566,acc=0.100000\n",
      "step 1130,train cost=0.000521,acc=0.180000\n",
      "step 1140,train cost=0.000558,acc=0.160000\n",
      "step 1150,train cost=0.000549,acc=0.120000\n",
      "step 1160,train cost=0.000589,acc=0.110000\n",
      "step 1170,train cost=0.000519,acc=0.140000\n",
      "step 1180,train cost=0.000512,acc=0.110000\n",
      "step 1190,train cost=0.000540,acc=0.130000\n",
      "step 1200,train cost=0.000541,acc=0.100000\n",
      "step 1210,train cost=0.000519,acc=0.140000\n",
      "step 1220,train cost=0.000528,acc=0.160000\n",
      "step 1230,train cost=0.000542,acc=0.150000\n",
      "step 1240,train cost=0.000540,acc=0.100000\n",
      "step 1250,train cost=0.000553,acc=0.130000\n",
      "step 1260,train cost=0.000567,acc=0.170000\n",
      "step 1270,train cost=0.000541,acc=0.100000\n",
      "step 1280,train cost=0.000548,acc=0.110000\n",
      "step 1290,train cost=0.000541,acc=0.100000\n",
      "step 1300,train cost=0.000530,acc=0.140000\n",
      "step 1310,train cost=0.000516,acc=0.170000\n",
      "step 1320,train cost=0.000536,acc=0.120000\n",
      "step 1330,train cost=0.000515,acc=0.180000\n",
      "step 1340,train cost=0.000541,acc=0.160000\n",
      "step 1350,train cost=0.000565,acc=0.080000\n",
      "step 1360,train cost=0.000484,acc=0.210000\n",
      "step 1370,train cost=0.000551,acc=0.120000\n",
      "step 1380,train cost=0.000527,acc=0.120000\n",
      "step 1390,train cost=0.000529,acc=0.080000\n",
      "step 1400,train cost=0.000516,acc=0.190000\n",
      "step 1410,train cost=0.000509,acc=0.180000\n",
      "step 1420,train cost=0.000529,acc=0.150000\n",
      "step 1430,train cost=0.000496,acc=0.120000\n",
      "step 1440,train cost=0.000519,acc=0.100000\n",
      "step 1450,train cost=0.000468,acc=0.200000\n",
      "step 1460,train cost=0.000532,acc=0.170000\n",
      "step 1470,train cost=0.000511,acc=0.190000\n",
      "step 1480,train cost=0.000514,acc=0.180000\n",
      "step 1490,train cost=0.000582,acc=0.120000\n",
      "step 1500,train cost=0.000536,acc=0.130000\n",
      "step 1510,train cost=0.000523,acc=0.180000\n",
      "step 1520,train cost=0.000476,acc=0.190000\n",
      "step 1530,train cost=0.000532,acc=0.080000\n",
      "step 1540,train cost=0.000487,acc=0.180000\n",
      "step 1550,train cost=0.000437,acc=0.230000\n",
      "step 1560,train cost=0.000470,acc=0.210000\n",
      "step 1570,train cost=0.000511,acc=0.140000\n",
      "step 1580,train cost=0.000516,acc=0.220000\n",
      "step 1590,train cost=0.000532,acc=0.130000\n",
      "step 1600,train cost=0.000485,acc=0.160000\n",
      "step 1610,train cost=0.000512,acc=0.100000\n",
      "step 1620,train cost=0.000520,acc=0.130000\n",
      "step 1630,train cost=0.000495,acc=0.150000\n",
      "step 1640,train cost=0.000506,acc=0.150000\n",
      "step 1650,train cost=0.000517,acc=0.130000\n",
      "step 1660,train cost=0.000487,acc=0.200000\n",
      "step 1670,train cost=0.000482,acc=0.160000\n",
      "step 1680,train cost=0.000507,acc=0.110000\n",
      "step 1690,train cost=0.000505,acc=0.160000\n",
      "step 1700,train cost=0.000517,acc=0.170000\n",
      "step 1710,train cost=0.000465,acc=0.190000\n",
      "step 1720,train cost=0.000544,acc=0.110000\n",
      "step 1730,train cost=0.000420,acc=0.190000\n",
      "step 1740,train cost=0.000483,acc=0.180000\n",
      "step 1750,train cost=0.000455,acc=0.220000\n",
      "step 1760,train cost=0.000436,acc=0.190000\n",
      "step 1770,train cost=0.000515,acc=0.100000\n",
      "step 1780,train cost=0.000534,acc=0.160000\n",
      "step 1790,train cost=0.000501,acc=0.190000\n",
      "step 1800,train cost=0.000497,acc=0.160000\n",
      "step 1810,train cost=0.000507,acc=0.130000\n",
      "step 1820,train cost=0.000496,acc=0.160000\n",
      "step 1830,train cost=0.000457,acc=0.190000\n",
      "step 1840,train cost=0.000435,acc=0.150000\n",
      "step 1850,train cost=0.000479,acc=0.110000\n",
      "step 1860,train cost=0.000475,acc=0.150000\n",
      "step 1870,train cost=0.000495,acc=0.170000\n",
      "step 1880,train cost=0.000448,acc=0.190000\n",
      "step 1890,train cost=0.000482,acc=0.190000\n",
      "step 1900,train cost=0.000427,acc=0.190000\n",
      "step 1910,train cost=0.000454,acc=0.130000\n",
      "step 1920,train cost=0.000522,acc=0.120000\n",
      "step 1930,train cost=0.000489,acc=0.200000\n",
      "step 1940,train cost=0.000488,acc=0.110000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1950,train cost=0.000478,acc=0.170000\n",
      "step 1960,train cost=0.000505,acc=0.180000\n",
      "step 1970,train cost=0.000457,acc=0.190000\n",
      "step 1980,train cost=0.000422,acc=0.210000\n",
      "step 1990,train cost=0.000448,acc=0.220000\n",
      "step 2000,train cost=0.000437,acc=0.180000\n",
      "step 2010,train cost=0.000440,acc=0.120000\n",
      "step 2020,train cost=0.000469,acc=0.150000\n",
      "step 2030,train cost=0.000427,acc=0.240000\n",
      "step 2040,train cost=0.000475,acc=0.150000\n",
      "step 2050,train cost=0.000453,acc=0.170000\n",
      "step 2060,train cost=0.000464,acc=0.160000\n",
      "step 2070,train cost=0.000460,acc=0.200000\n",
      "step 2080,train cost=0.000441,acc=0.160000\n",
      "step 2090,train cost=0.000482,acc=0.140000\n",
      "step 2100,train cost=0.000429,acc=0.170000\n",
      "step 2110,train cost=0.000474,acc=0.140000\n",
      "step 2120,train cost=0.000461,acc=0.120000\n",
      "step 2130,train cost=0.000474,acc=0.140000\n",
      "step 2140,train cost=0.000443,acc=0.170000\n",
      "step 2150,train cost=0.000437,acc=0.200000\n",
      "step 2160,train cost=0.000470,acc=0.190000\n",
      "step 2170,train cost=0.000436,acc=0.150000\n",
      "step 2180,train cost=0.000446,acc=0.130000\n",
      "step 2190,train cost=0.000417,acc=0.200000\n",
      "step 2200,train cost=0.000471,acc=0.150000\n",
      "step 2210,train cost=0.000429,acc=0.180000\n",
      "step 2220,train cost=0.000379,acc=0.240000\n",
      "step 2230,train cost=0.000427,acc=0.210000\n",
      "step 2240,train cost=0.000458,acc=0.170000\n",
      "step 2250,train cost=0.000429,acc=0.190000\n",
      "step 2260,train cost=0.000435,acc=0.160000\n",
      "step 2270,train cost=0.000403,acc=0.200000\n",
      "step 2280,train cost=0.000417,acc=0.250000\n",
      "step 2290,train cost=0.000397,acc=0.220000\n",
      "step 2300,train cost=0.000424,acc=0.220000\n",
      "step 2310,train cost=0.000407,acc=0.210000\n",
      "step 2320,train cost=0.000402,acc=0.200000\n",
      "step 2330,train cost=0.000410,acc=0.270000\n",
      "step 2340,train cost=0.000402,acc=0.170000\n",
      "step 2350,train cost=0.000396,acc=0.230000\n",
      "step 2360,train cost=0.000376,acc=0.260000\n",
      "step 2370,train cost=0.000375,acc=0.230000\n",
      "step 2380,train cost=0.000417,acc=0.200000\n",
      "step 2390,train cost=0.000369,acc=0.280000\n",
      "step 2400,train cost=0.000363,acc=0.260000\n",
      "step 2410,train cost=0.000435,acc=0.270000\n",
      "step 2420,train cost=0.000419,acc=0.190000\n",
      "step 2430,train cost=0.000424,acc=0.180000\n",
      "step 2440,train cost=0.000377,acc=0.230000\n",
      "step 2450,train cost=0.000393,acc=0.190000\n",
      "step 2460,train cost=0.000412,acc=0.140000\n",
      "step 2470,train cost=0.000383,acc=0.290000\n",
      "step 2480,train cost=0.000392,acc=0.220000\n",
      "step 2490,train cost=0.000361,acc=0.280000\n",
      "step 2500,train cost=0.000384,acc=0.200000\n",
      "step 2510,train cost=0.000401,acc=0.210000\n",
      "step 2520,train cost=0.000405,acc=0.240000\n",
      "step 2530,train cost=0.000412,acc=0.190000\n",
      "step 2540,train cost=0.000430,acc=0.170000\n",
      "step 2550,train cost=0.000328,acc=0.280000\n",
      "step 2560,train cost=0.000363,acc=0.250000\n",
      "step 2570,train cost=0.000342,acc=0.280000\n",
      "step 2580,train cost=0.000418,acc=0.180000\n",
      "step 2590,train cost=0.000374,acc=0.310000\n",
      "step 2600,train cost=0.000379,acc=0.280000\n",
      "step 2610,train cost=0.000408,acc=0.220000\n",
      "step 2620,train cost=0.000380,acc=0.240000\n",
      "step 2630,train cost=0.000347,acc=0.290000\n",
      "step 2640,train cost=0.000330,acc=0.330000\n",
      "step 2650,train cost=0.000403,acc=0.210000\n",
      "step 2660,train cost=0.000341,acc=0.210000\n",
      "step 2670,train cost=0.000387,acc=0.200000\n",
      "step 2680,train cost=0.000387,acc=0.210000\n",
      "step 2690,train cost=0.000344,acc=0.300000\n",
      "step 2700,train cost=0.000370,acc=0.250000\n",
      "step 2710,train cost=0.000336,acc=0.360000\n",
      "step 2720,train cost=0.000384,acc=0.220000\n",
      "step 2730,train cost=0.000385,acc=0.210000\n",
      "step 2740,train cost=0.000343,acc=0.290000\n",
      "step 2750,train cost=0.000332,acc=0.360000\n",
      "step 2760,train cost=0.000378,acc=0.250000\n",
      "step 2770,train cost=0.000372,acc=0.300000\n",
      "step 2780,train cost=0.000350,acc=0.260000\n",
      "step 2790,train cost=0.000333,acc=0.360000\n",
      "step 2800,train cost=0.000359,acc=0.300000\n",
      "step 2810,train cost=0.000306,acc=0.320000\n",
      "step 2820,train cost=0.000346,acc=0.280000\n",
      "step 2830,train cost=0.000351,acc=0.320000\n",
      "step 2840,train cost=0.000362,acc=0.200000\n",
      "step 2850,train cost=0.000344,acc=0.240000\n",
      "step 2860,train cost=0.000320,acc=0.320000\n",
      "step 2870,train cost=0.000316,acc=0.370000\n",
      "step 2880,train cost=0.000326,acc=0.310000\n",
      "step 2890,train cost=0.000329,acc=0.290000\n",
      "step 2900,train cost=0.000294,acc=0.330000\n",
      "step 2910,train cost=0.000342,acc=0.310000\n",
      "step 2920,train cost=0.000311,acc=0.390000\n",
      "step 2930,train cost=0.000381,acc=0.240000\n",
      "step 2940,train cost=0.000340,acc=0.290000\n",
      "step 2950,train cost=0.000309,acc=0.350000\n",
      "step 2960,train cost=0.000328,acc=0.290000\n",
      "step 2970,train cost=0.000344,acc=0.280000\n",
      "step 2980,train cost=0.000310,acc=0.370000\n",
      "step 2990,train cost=0.000348,acc=0.350000\n",
      "step 3000,train cost=0.000302,acc=0.370000\n",
      "step 3010,train cost=0.000323,acc=0.300000\n",
      "step 3020,train cost=0.000301,acc=0.370000\n",
      "step 3030,train cost=0.000305,acc=0.350000\n",
      "step 3040,train cost=0.000296,acc=0.330000\n",
      "step 3050,train cost=0.000306,acc=0.300000\n",
      "step 3060,train cost=0.000311,acc=0.390000\n",
      "step 3070,train cost=0.000242,acc=0.390000\n",
      "step 3080,train cost=0.000308,acc=0.310000\n",
      "step 3090,train cost=0.000286,acc=0.390000\n",
      "step 3100,train cost=0.000264,acc=0.420000\n",
      "step 3110,train cost=0.000313,acc=0.380000\n",
      "step 3120,train cost=0.000303,acc=0.290000\n",
      "step 3130,train cost=0.000294,acc=0.360000\n",
      "step 3140,train cost=0.000300,acc=0.330000\n",
      "step 3150,train cost=0.000276,acc=0.320000\n",
      "step 3160,train cost=0.000294,acc=0.390000\n",
      "step 3170,train cost=0.000233,acc=0.460000\n",
      "step 3180,train cost=0.000252,acc=0.490000\n",
      "step 3190,train cost=0.000291,acc=0.370000\n",
      "step 3200,train cost=0.000276,acc=0.470000\n",
      "step 3210,train cost=0.000278,acc=0.310000\n",
      "step 3220,train cost=0.000291,acc=0.320000\n",
      "step 3230,train cost=0.000282,acc=0.370000\n",
      "step 3240,train cost=0.000241,acc=0.420000\n",
      "step 3250,train cost=0.000234,acc=0.490000\n",
      "step 3260,train cost=0.000246,acc=0.490000\n",
      "step 3270,train cost=0.000254,acc=0.400000\n",
      "step 3280,train cost=0.000256,acc=0.380000\n",
      "step 3290,train cost=0.000298,acc=0.340000\n",
      "step 3300,train cost=0.000284,acc=0.370000\n",
      "step 3310,train cost=0.000276,acc=0.390000\n",
      "step 3320,train cost=0.000248,acc=0.480000\n",
      "step 3330,train cost=0.000260,acc=0.400000\n",
      "step 3340,train cost=0.000245,acc=0.400000\n",
      "step 3350,train cost=0.000209,acc=0.530000\n",
      "step 3360,train cost=0.000258,acc=0.380000\n",
      "step 3370,train cost=0.000291,acc=0.330000\n",
      "step 3380,train cost=0.000259,acc=0.480000\n",
      "step 3390,train cost=0.000237,acc=0.420000\n",
      "step 3400,train cost=0.000262,acc=0.440000\n",
      "step 3410,train cost=0.000291,acc=0.360000\n",
      "step 3420,train cost=0.000260,acc=0.360000\n",
      "step 3430,train cost=0.000242,acc=0.480000\n",
      "step 3440,train cost=0.000283,acc=0.420000\n",
      "step 3450,train cost=0.000243,acc=0.460000\n",
      "step 3460,train cost=0.000231,acc=0.510000\n",
      "step 3470,train cost=0.000227,acc=0.520000\n",
      "step 3480,train cost=0.000232,acc=0.520000\n",
      "step 3490,train cost=0.000238,acc=0.460000\n",
      "step 3500,train cost=0.000210,acc=0.590000\n",
      "step 3510,train cost=0.000233,acc=0.430000\n",
      "step 3520,train cost=0.000259,acc=0.390000\n",
      "step 3530,train cost=0.000201,acc=0.550000\n",
      "step 3540,train cost=0.000224,acc=0.540000\n",
      "step 3550,train cost=0.000269,acc=0.420000\n",
      "step 3560,train cost=0.000260,acc=0.480000\n",
      "step 3570,train cost=0.000233,acc=0.460000\n",
      "step 3580,train cost=0.000237,acc=0.510000\n",
      "step 3590,train cost=0.000229,acc=0.500000\n",
      "step 3600,train cost=0.000204,acc=0.560000\n",
      "step 3610,train cost=0.000249,acc=0.450000\n",
      "step 3620,train cost=0.000244,acc=0.440000\n",
      "step 3630,train cost=0.000246,acc=0.450000\n",
      "step 3640,train cost=0.000245,acc=0.490000\n",
      "step 3650,train cost=0.000198,acc=0.520000\n",
      "step 3660,train cost=0.000233,acc=0.470000\n",
      "step 3670,train cost=0.000210,acc=0.520000\n",
      "step 3680,train cost=0.000224,acc=0.460000\n",
      "step 3690,train cost=0.000199,acc=0.530000\n",
      "step 3700,train cost=0.000213,acc=0.470000\n",
      "step 3710,train cost=0.000207,acc=0.540000\n",
      "step 3720,train cost=0.000210,acc=0.570000\n",
      "step 3730,train cost=0.000222,acc=0.440000\n",
      "step 3740,train cost=0.000234,acc=0.390000\n",
      "step 3750,train cost=0.000218,acc=0.500000\n",
      "step 3760,train cost=0.000202,acc=0.550000\n",
      "step 3770,train cost=0.000209,acc=0.480000\n",
      "step 3780,train cost=0.000196,acc=0.480000\n",
      "step 3790,train cost=0.000212,acc=0.470000\n",
      "step 3800,train cost=0.000205,acc=0.480000\n",
      "step 3810,train cost=0.000163,acc=0.630000\n",
      "step 3820,train cost=0.000204,acc=0.490000\n",
      "step 3830,train cost=0.000169,acc=0.610000\n",
      "step 3840,train cost=0.000244,acc=0.440000\n",
      "step 3850,train cost=0.000190,acc=0.590000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3860,train cost=0.000252,acc=0.450000\n",
      "step 3870,train cost=0.000181,acc=0.550000\n",
      "step 3880,train cost=0.000205,acc=0.480000\n",
      "step 3890,train cost=0.000184,acc=0.590000\n",
      "step 3900,train cost=0.000192,acc=0.560000\n",
      "step 3910,train cost=0.000200,acc=0.570000\n",
      "step 3920,train cost=0.000195,acc=0.540000\n",
      "step 3930,train cost=0.000181,acc=0.620000\n",
      "step 3940,train cost=0.000212,acc=0.590000\n",
      "step 3950,train cost=0.000198,acc=0.560000\n",
      "step 3960,train cost=0.000228,acc=0.520000\n",
      "step 3970,train cost=0.000181,acc=0.550000\n",
      "step 3980,train cost=0.000204,acc=0.530000\n",
      "step 3990,train cost=0.000206,acc=0.460000\n",
      "step 4000,train cost=0.000209,acc=0.520000\n",
      "step 4010,train cost=0.000183,acc=0.620000\n",
      "step 4020,train cost=0.000142,acc=0.630000\n",
      "step 4030,train cost=0.000215,acc=0.560000\n",
      "step 4040,train cost=0.000188,acc=0.560000\n",
      "step 4050,train cost=0.000166,acc=0.630000\n",
      "step 4060,train cost=0.000183,acc=0.580000\n",
      "step 4070,train cost=0.000193,acc=0.570000\n",
      "step 4080,train cost=0.000176,acc=0.590000\n",
      "step 4090,train cost=0.000153,acc=0.560000\n",
      "step 4100,train cost=0.000148,acc=0.580000\n",
      "step 4110,train cost=0.000189,acc=0.540000\n",
      "step 4120,train cost=0.000209,acc=0.510000\n",
      "step 4130,train cost=0.000151,acc=0.620000\n",
      "step 4140,train cost=0.000166,acc=0.620000\n",
      "step 4150,train cost=0.000143,acc=0.670000\n",
      "step 4160,train cost=0.000155,acc=0.560000\n",
      "step 4170,train cost=0.000148,acc=0.550000\n",
      "step 4180,train cost=0.000165,acc=0.610000\n",
      "step 4190,train cost=0.000168,acc=0.610000\n",
      "step 4200,train cost=0.000171,acc=0.580000\n",
      "step 4210,train cost=0.000152,acc=0.650000\n",
      "step 4220,train cost=0.000160,acc=0.590000\n",
      "step 4230,train cost=0.000139,acc=0.660000\n",
      "step 4240,train cost=0.000153,acc=0.610000\n",
      "step 4250,train cost=0.000195,acc=0.500000\n",
      "step 4260,train cost=0.000191,acc=0.560000\n",
      "step 4270,train cost=0.000135,acc=0.660000\n",
      "step 4280,train cost=0.000151,acc=0.640000\n",
      "step 4290,train cost=0.000149,acc=0.660000\n",
      "step 4300,train cost=0.000159,acc=0.610000\n",
      "step 4310,train cost=0.000162,acc=0.620000\n",
      "step 4320,train cost=0.000130,acc=0.730000\n",
      "step 4330,train cost=0.000138,acc=0.630000\n",
      "step 4340,train cost=0.000156,acc=0.620000\n",
      "step 4350,train cost=0.000150,acc=0.660000\n",
      "step 4360,train cost=0.000137,acc=0.660000\n",
      "step 4370,train cost=0.000174,acc=0.610000\n",
      "step 4380,train cost=0.000191,acc=0.560000\n",
      "step 4390,train cost=0.000160,acc=0.660000\n",
      "step 4400,train cost=0.000145,acc=0.640000\n",
      "step 4410,train cost=0.000166,acc=0.600000\n",
      "step 4420,train cost=0.000132,acc=0.710000\n",
      "step 4430,train cost=0.000182,acc=0.560000\n",
      "step 4440,train cost=0.000144,acc=0.630000\n",
      "step 4450,train cost=0.000126,acc=0.720000\n",
      "step 4460,train cost=0.000152,acc=0.650000\n",
      "step 4470,train cost=0.000115,acc=0.750000\n",
      "step 4480,train cost=0.000146,acc=0.630000\n",
      "step 4490,train cost=0.000153,acc=0.610000\n",
      "step 4500,train cost=0.000130,acc=0.660000\n",
      "step 4510,train cost=0.000162,acc=0.610000\n",
      "step 4520,train cost=0.000135,acc=0.660000\n",
      "step 4530,train cost=0.000126,acc=0.710000\n",
      "step 4540,train cost=0.000141,acc=0.660000\n",
      "step 4550,train cost=0.000146,acc=0.630000\n",
      "step 4560,train cost=0.000161,acc=0.600000\n",
      "step 4570,train cost=0.000121,acc=0.700000\n",
      "step 4580,train cost=0.000115,acc=0.710000\n",
      "step 4590,train cost=0.000146,acc=0.670000\n",
      "step 4600,train cost=0.000154,acc=0.640000\n",
      "step 4610,train cost=0.000147,acc=0.560000\n",
      "step 4620,train cost=0.000102,acc=0.800000\n",
      "step 4630,train cost=0.000133,acc=0.680000\n",
      "step 4640,train cost=0.000108,acc=0.690000\n",
      "step 4650,train cost=0.000144,acc=0.670000\n",
      "step 4660,train cost=0.000115,acc=0.740000\n",
      "step 4670,train cost=0.000123,acc=0.720000\n",
      "step 4680,train cost=0.000135,acc=0.690000\n",
      "step 4690,train cost=0.000123,acc=0.690000\n",
      "step 4700,train cost=0.000109,acc=0.690000\n",
      "step 4710,train cost=0.000119,acc=0.700000\n",
      "step 4720,train cost=0.000119,acc=0.720000\n",
      "step 4730,train cost=0.000135,acc=0.670000\n",
      "step 4740,train cost=0.000105,acc=0.710000\n",
      "step 4750,train cost=0.000109,acc=0.710000\n",
      "step 4760,train cost=0.000121,acc=0.720000\n",
      "step 4770,train cost=0.000150,acc=0.610000\n",
      "step 4780,train cost=0.000120,acc=0.670000\n",
      "step 4790,train cost=0.000111,acc=0.700000\n",
      "step 4800,train cost=0.000132,acc=0.650000\n",
      "step 4810,train cost=0.000114,acc=0.760000\n",
      "step 4820,train cost=0.000115,acc=0.700000\n",
      "step 4830,train cost=0.000098,acc=0.760000\n",
      "step 4840,train cost=0.000101,acc=0.790000\n",
      "step 4850,train cost=0.000127,acc=0.680000\n",
      "step 4860,train cost=0.000106,acc=0.700000\n",
      "step 4870,train cost=0.000089,acc=0.760000\n",
      "step 4880,train cost=0.000110,acc=0.700000\n",
      "step 4890,train cost=0.000123,acc=0.670000\n",
      "step 4900,train cost=0.000125,acc=0.710000\n",
      "step 4910,train cost=0.000107,acc=0.740000\n",
      "step 4920,train cost=0.000106,acc=0.740000\n",
      "step 4930,train cost=0.000103,acc=0.690000\n",
      "step 4940,train cost=0.000132,acc=0.650000\n",
      "step 4950,train cost=0.000133,acc=0.620000\n",
      "step 4960,train cost=0.000114,acc=0.740000\n",
      "step 4970,train cost=0.000093,acc=0.760000\n",
      "step 4980,train cost=0.000142,acc=0.660000\n",
      "step 4990,train cost=0.000103,acc=0.760000\n",
      "step 5000,train cost=0.000095,acc=0.780000\n",
      "step 5010,train cost=0.000080,acc=0.820000\n",
      "step 5020,train cost=0.000111,acc=0.680000\n",
      "step 5030,train cost=0.000086,acc=0.790000\n",
      "step 5040,train cost=0.000107,acc=0.720000\n",
      "step 5050,train cost=0.000104,acc=0.740000\n",
      "step 5060,train cost=0.000076,acc=0.810000\n",
      "step 5070,train cost=0.000101,acc=0.760000\n",
      "step 5080,train cost=0.000073,acc=0.860000\n",
      "step 5090,train cost=0.000142,acc=0.650000\n",
      "step 5100,train cost=0.000093,acc=0.780000\n",
      "step 5110,train cost=0.000090,acc=0.740000\n",
      "step 5120,train cost=0.000116,acc=0.710000\n",
      "step 5130,train cost=0.000084,acc=0.790000\n",
      "step 5140,train cost=0.000099,acc=0.760000\n",
      "step 5150,train cost=0.000090,acc=0.780000\n",
      "step 5160,train cost=0.000088,acc=0.810000\n",
      "step 5170,train cost=0.000086,acc=0.800000\n",
      "step 5180,train cost=0.000099,acc=0.720000\n",
      "step 5190,train cost=0.000087,acc=0.770000\n",
      "step 5200,train cost=0.000103,acc=0.720000\n",
      "step 5210,train cost=0.000084,acc=0.810000\n",
      "step 5220,train cost=0.000064,acc=0.850000\n",
      "step 5230,train cost=0.000092,acc=0.730000\n",
      "step 5240,train cost=0.000083,acc=0.770000\n",
      "step 5250,train cost=0.000084,acc=0.810000\n",
      "step 5260,train cost=0.000091,acc=0.760000\n",
      "step 5270,train cost=0.000084,acc=0.820000\n",
      "step 5280,train cost=0.000092,acc=0.740000\n",
      "step 5290,train cost=0.000068,acc=0.840000\n",
      "step 5300,train cost=0.000075,acc=0.790000\n",
      "step 5310,train cost=0.000076,acc=0.750000\n",
      "step 5320,train cost=0.000085,acc=0.810000\n",
      "step 5330,train cost=0.000066,acc=0.790000\n",
      "step 5340,train cost=0.000090,acc=0.770000\n",
      "step 5350,train cost=0.000096,acc=0.790000\n",
      "step 5360,train cost=0.000067,acc=0.840000\n",
      "step 5370,train cost=0.000069,acc=0.850000\n",
      "step 5380,train cost=0.000081,acc=0.800000\n",
      "step 5390,train cost=0.000077,acc=0.770000\n",
      "step 5400,train cost=0.000098,acc=0.730000\n",
      "step 5410,train cost=0.000095,acc=0.760000\n",
      "step 5420,train cost=0.000062,acc=0.800000\n",
      "step 5430,train cost=0.000098,acc=0.780000\n",
      "step 5440,train cost=0.000083,acc=0.790000\n",
      "step 5450,train cost=0.000100,acc=0.750000\n",
      "step 5460,train cost=0.000076,acc=0.820000\n",
      "step 5470,train cost=0.000073,acc=0.830000\n",
      "step 5480,train cost=0.000067,acc=0.830000\n",
      "step 5490,train cost=0.000080,acc=0.790000\n",
      "step 5500,train cost=0.000075,acc=0.830000\n",
      "step 5510,train cost=0.000088,acc=0.820000\n",
      "step 5520,train cost=0.000069,acc=0.840000\n",
      "step 5530,train cost=0.000074,acc=0.780000\n",
      "step 5540,train cost=0.000088,acc=0.780000\n",
      "step 5550,train cost=0.000107,acc=0.780000\n",
      "step 5560,train cost=0.000060,acc=0.860000\n",
      "step 5570,train cost=0.000060,acc=0.800000\n",
      "step 5580,train cost=0.000068,acc=0.850000\n",
      "step 5590,train cost=0.000085,acc=0.730000\n",
      "step 5600,train cost=0.000058,acc=0.870000\n",
      "step 5610,train cost=0.000069,acc=0.830000\n",
      "step 5620,train cost=0.000061,acc=0.830000\n",
      "step 5630,train cost=0.000072,acc=0.800000\n",
      "step 5640,train cost=0.000092,acc=0.760000\n",
      "step 5650,train cost=0.000062,acc=0.820000\n",
      "step 5660,train cost=0.000069,acc=0.820000\n",
      "step 5670,train cost=0.000072,acc=0.820000\n",
      "step 5680,train cost=0.000078,acc=0.810000\n",
      "step 5690,train cost=0.000067,acc=0.820000\n",
      "step 5700,train cost=0.000085,acc=0.800000\n",
      "step 5710,train cost=0.000060,acc=0.880000\n",
      "step 5720,train cost=0.000056,acc=0.870000\n",
      "step 5730,train cost=0.000056,acc=0.850000\n",
      "step 5740,train cost=0.000059,acc=0.860000\n",
      "step 5750,train cost=0.000069,acc=0.830000\n",
      "step 5760,train cost=0.000074,acc=0.840000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5770,train cost=0.000057,acc=0.860000\n",
      "step 5780,train cost=0.000070,acc=0.830000\n",
      "step 5790,train cost=0.000067,acc=0.850000\n",
      "step 5800,train cost=0.000068,acc=0.850000\n",
      "step 5810,train cost=0.000064,acc=0.850000\n",
      "step 5820,train cost=0.000074,acc=0.820000\n",
      "step 5830,train cost=0.000062,acc=0.810000\n",
      "step 5840,train cost=0.000052,acc=0.880000\n",
      "step 5850,train cost=0.000060,acc=0.860000\n",
      "step 5860,train cost=0.000067,acc=0.820000\n",
      "step 5870,train cost=0.000049,acc=0.840000\n",
      "step 5880,train cost=0.000051,acc=0.860000\n",
      "step 5890,train cost=0.000063,acc=0.830000\n",
      "step 5900,train cost=0.000085,acc=0.790000\n",
      "step 5910,train cost=0.000051,acc=0.880000\n",
      "step 5920,train cost=0.000054,acc=0.840000\n",
      "step 5930,train cost=0.000052,acc=0.860000\n",
      "step 5940,train cost=0.000059,acc=0.850000\n",
      "step 5950,train cost=0.000054,acc=0.910000\n",
      "step 5960,train cost=0.000061,acc=0.850000\n",
      "step 5970,train cost=0.000062,acc=0.840000\n",
      "step 5980,train cost=0.000057,acc=0.850000\n",
      "step 5990,train cost=0.000038,acc=0.970000\n",
      "step 6000,train cost=0.000056,acc=0.870000\n",
      "step 6010,train cost=0.000066,acc=0.810000\n",
      "step 6020,train cost=0.000065,acc=0.820000\n",
      "step 6030,train cost=0.000049,acc=0.880000\n",
      "step 6040,train cost=0.000050,acc=0.860000\n",
      "step 6050,train cost=0.000051,acc=0.870000\n",
      "step 6060,train cost=0.000070,acc=0.780000\n",
      "step 6070,train cost=0.000063,acc=0.840000\n",
      "step 6080,train cost=0.000061,acc=0.880000\n",
      "step 6090,train cost=0.000047,acc=0.880000\n",
      "step 6100,train cost=0.000065,acc=0.820000\n",
      "step 6110,train cost=0.000070,acc=0.830000\n",
      "step 6120,train cost=0.000056,acc=0.860000\n",
      "step 6130,train cost=0.000064,acc=0.840000\n",
      "step 6140,train cost=0.000052,acc=0.870000\n",
      "step 6150,train cost=0.000046,acc=0.890000\n",
      "step 6160,train cost=0.000043,acc=0.930000\n",
      "step 6170,train cost=0.000053,acc=0.850000\n",
      "step 6180,train cost=0.000060,acc=0.870000\n",
      "step 6190,train cost=0.000046,acc=0.860000\n",
      "step 6200,train cost=0.000058,acc=0.880000\n",
      "step 6210,train cost=0.000056,acc=0.840000\n",
      "step 6220,train cost=0.000057,acc=0.840000\n",
      "step 6230,train cost=0.000054,acc=0.870000\n",
      "step 6240,train cost=0.000046,acc=0.890000\n",
      "step 6250,train cost=0.000046,acc=0.880000\n",
      "step 6260,train cost=0.000054,acc=0.850000\n",
      "step 6270,train cost=0.000061,acc=0.860000\n",
      "step 6280,train cost=0.000053,acc=0.850000\n",
      "step 6290,train cost=0.000044,acc=0.880000\n",
      "step 6300,train cost=0.000064,acc=0.820000\n",
      "step 6310,train cost=0.000043,acc=0.890000\n",
      "step 6320,train cost=0.000061,acc=0.860000\n",
      "step 6330,train cost=0.000050,acc=0.890000\n",
      "step 6340,train cost=0.000048,acc=0.870000\n",
      "step 6350,train cost=0.000054,acc=0.860000\n",
      "step 6360,train cost=0.000047,acc=0.890000\n",
      "step 6370,train cost=0.000059,acc=0.840000\n",
      "step 6380,train cost=0.000043,acc=0.890000\n",
      "step 6390,train cost=0.000046,acc=0.880000\n",
      "step 6400,train cost=0.000051,acc=0.870000\n",
      "step 6410,train cost=0.000061,acc=0.840000\n",
      "step 6420,train cost=0.000035,acc=0.910000\n",
      "step 6430,train cost=0.000047,acc=0.880000\n",
      "step 6440,train cost=0.000057,acc=0.850000\n",
      "step 6450,train cost=0.000043,acc=0.860000\n",
      "step 6460,train cost=0.000047,acc=0.900000\n",
      "step 6470,train cost=0.000044,acc=0.910000\n",
      "step 6480,train cost=0.000039,acc=0.900000\n",
      "step 6490,train cost=0.000043,acc=0.920000\n",
      "step 6500,train cost=0.000040,acc=0.910000\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "for i in range(6500):\n",
    "    _batch_size=100\n",
    "    X_batch, y_batch = get_next_batch(_batch_size)\n",
    "    cost, acc, _ = sess.run([loss,accuracy,train_op],feed_dict={X: X_batch, y_input:y_batch,keep_prob:0.5, batch_size: _batch_size})\n",
    "    if (i+1)%10 == 0:\n",
    "        print(\"step {},train cost={:.6f},acc={:.6f}\".format(i+1,cost,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the next predict word's index, depend on the cumsum of probability \n",
    "def get_next_index(vector):\n",
    "    threshold = np.random.rand(1)\n",
    "    #print(\"threshold is:\",threshold)\n",
    "    i = 0\n",
    "    proba_sum = 0.0\n",
    "    while(i<len(vector)):\n",
    "        proba_sum += vector[i]\n",
    "        if(proba_sum>threshold):\n",
    "            #print(\"Find i:\",i)\n",
    "            break\n",
    "        i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push Old Baggins to write more about the quest \n",
    "def Fake_Old_Baggins(len_to_write):\n",
    "    #text = [\"gandalf\",\"was\",\"reading\",\"a\",\"riddle\",\"written\",\"in\"]\n",
    "    #kickoff_phrase = [\"gandalf\",\"was\",\"reading\",\"a\",\"riddle\",\"written\",\"in\"]\n",
    "    text = [\"gollum\",\"was\",\"catching\",\"fish\",\"by\",\"the\",\"lake\"]\n",
    "    kickoff_phrase = [\"gollum\",\"was\",\"catching\",\"fish\",\"by\",\"the\",\"lake\"]\n",
    "    i = 0\n",
    "    while(i < len_to_write):\n",
    "        kickoff_vector = []\n",
    "        for j in range(timestep_size):\n",
    "            kickoff_vector.append(w2v.wv.get_vector(kickoff_phrase[j]))\n",
    "        kickoff_batch = []\n",
    "        kickoff_batch.append(kickoff_vector)\n",
    "        kickoff_batch = np.array(kickoff_batch)\n",
    "        \n",
    "        predict_vector = sess.run([y_pre],feed_dict={X: kickoff_batch, keep_prob:1.0,batch_size: 1})\n",
    "        predict_vector = np.array(predict_vector)\n",
    "        predict_word = label_encoder.inverse_transform(get_next_index(predict_vector[0][0]))\n",
    "        \n",
    "        kickoff_phrase.pop(0)\n",
    "        kickoff_phrase.append(predict_word)\n",
    "        text.append(predict_word)\n",
    "        \n",
    "        #print(kickoff_phrase)\n",
    "        \n",
    "        i += 1\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gollum was catching fish by the lake of the younger of the shire , for they loved to grow after him . he left the into the mountain river , and right agin the old forest . that was a dark place place , until the fact of the wise could discover no more . but at last i can carry on the story , i think . 'fong after , it was his birthday at the time , but he still his 'precious collected many manuscripts written by scribes . the nine of all holes and still were mentioned . but they became to call the other . the book of the westlands were great and according to shire-folk bare with the same of large and delight . 'he , even that the hilly were the poorest of mordor , and they had crowded the after-dinner speech in hobbits . though the dates were usually to be seen . the original of hobbits lies had back up all and night . there was a great flash of light , and that was green in the light : there was his scratch echo . he whizzed alone - to himself : about he thought he comes it and he used it , if the wizard , and there was a gleam in his eyes . 'i think , bilbo , ' he said him , as i will do any , more than i were said . i how all by bilbo maimed size . for some years of them took ? ' 'yes gandalf . 'and i wonder the purchase . i saw often mr. bilbo 's first many : he would be a great , ' he said . ' 'i i am not to find . ' he thought with a commanding at the\n"
     ]
    }
   ],
   "source": [
    "rslt = Fake_Old_Baggins(300)\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gollum was catching fish by the lake of the younger of the shire . 'so it is , ' he said silent a old present . after not you 'll he told you with . ' 'i am not to see you visible , ' he said . 'and i wonder of course . he would lead him and let it away the back , he will -covered and anyone out . bilbo was round in the dark corner , as he tunnelled with her place . he was hardly pleased , and gathering and bilbo had once to report him . a first itself . to moment he wore it , and his hand , he had hungry on the first ; and he whistled three back . the last , was , but rather had ever to be bag valiant . bilbo was very mr. bilbo 'elves first seen on the truth from to a while in the world , and never neither to rohan of any in you reckon . who invented stories good-bye anyway a indoors . they my of hard dim . they began to their faces and bilbo had a seen springle-ring of his sancho , and my change , secured . 'there 's no saying fishy , the old was gathered , and a shadow . he had a feathered significant of young hobbits lived . though the elves of the wise were not noticeable with ours . their history is long long ordered audience , and the chief of the irresponsible could discover the sounds of s.r between their ears of peculiar an , nine no of the shire that had at last they had been trying to be destroyed years . the anniversary of locally had passed to them long from his sword like a shadow on the\n"
     ]
    }
   ],
   "source": [
    "rslt2 = Fake_Old_Baggins(300)\n",
    "print(rslt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gollum was catching fish by the lake of the younger generation of hobbits . 'ah , but he has likely enough been adding to what he brought at first , ' argued the miller , voicing common opinion . 'he 's often away from home . ' look to be you on ! ' he said , 'and n't you want , ' said frodo . 'i wanted the truth . ' he clutched away . he looked sternly back to his hole , and stood for a moment listening with a smile to the din in the pavilion and to the sounds of merrymaking in the dark , he put his hand on a ring , lying on the floor of a tunnel . ele it remained in his pocket . they seemed to him , and eat to be very . even if he was no a sword . ' the ring , ' said gandalf . 'do , that did you want of me all ? ' asked frodo . 'and i wonder in time . ' 'so i do n't know , ' said deal . 'i believe the truth . it was important . magic rings are - well , magical ; and they were rare and bootless . the stoors of building fell and barns , when large folk whom they do not wish to meet come blundering by ; and the an case is pleased recalled among the red book of men . the name just is the master he had given his long duty with and could make them off . ' they do not that did not know yet like machines . ' no he , was a few shock . the flights of the 1 the hobbits never , legend by years that been given to be\n"
     ]
    }
   ],
   "source": [
    "rslt3 = Fake_Old_Baggins(300)\n",
    "print(rslt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gollum was catching fish by the lake of the younger and the hobbits . but the hoard were not know to them . it was very trying at i , but i am to feel him . for my except . ' he asked . 'is , he , all , after angelica 's all an , and the scratched , but the flowering of the crossing could never come . he said to him , and 'yes goodbye all it had been been profit . bilbo 's frodo , ' he said , getting it is good , and whether it nor come , i am sure look , ' said gandalf . 'i do not make use of it , if i came . for a while mr. : he is not a cool part , and the council has come to speak . give me the ring for a moment . ' frodo , and i am if got to make it on again , ' he added with a look at the stranger , the miller miller , were a few branch with the high . bilbo was sitting , the young , keeping in his twelve 's cousin , and that mr. drogo , but you had never kept that before many dwarves and could aware after the borders . the bounders of rumours may have come . i think besides everything that permanent came to i am to , it picked seldom , and his wife disappearance was too 'shire-words and scribes in people . but in moderate the rings were the council of the shire won ; but i think to time too with it , and riches it had long spiders , and the authorities that it was very : his magic , and the council real . in his hand and he swam it , and the wizard passed , the hand was by the ores , and three of their that were taller . even he had been to think , at the first , i never heard after a good many for the forge . 'put of them , and a preference of anger , and they turned the faces . the sun was pippin , and the wind was in the south . everything looked fresh . he gone , that the one looked to the shire about bounders of the shire that at all he sought the guests , and he were the effects of the calendars of old ' was especially dashed , but removed . ' he was with it , and treachery the sky standstill to the warm in the south . the craft named to have been the first actual , of course . the original were legend to get the truth of shock and annoyance , and they turned deep and . the sun , was in the council . he felt that for the moment ; there in a shadow of a spears that blew it . suddenly he slipped up and returned his garden , and included him , and he used the thing that his very had ever had been killed to refuse it written from the morning , ' said gandalf . 'very old of the present . the party is not native to have been written by greater , grandson of devices , which still hid the names of gondor of the tale from aragorn they refer the brandybucks in mud , to the last of the party . 'run it was in a moment seen that since all all after the fair of the family , who had been later the ring before many years , and used it likely hoard in them , they were other troubled of ores , and nibbling in s.r , that they had felt in less , as in which they can not be seen . but the spite family in the shire ; but preceded his ancient business were with him . but at the 'attractions time had been to it , arrange under his face . a kindly of light rising was at a standing . the light deal was now . this was too . it came into the great , the teachers , which dark character of planning of names that unimportant from the truth . bilbo by the boat of his hand ' there was a residence and the mr. families . the light knew it was too for it ; and he and deal ever that had come to live it at the shire , gandalf came with with an , and gurgling in a handful . me-in concerning in loneliness , for the crumbs of the crossing of the ring was so called because it was long preserved at undertowers , the home of the fairbairns . wardens the ring of the party he was very . it was already his master master , but he was 'you by him . ' he retired the door , and back the last , outside on the hill of a tunnel . ele all big . ' 'maybe , ' said gandalf , laughing them to the full with , a hand holiday in the east , and still the gladden of them 's between it was too smjagol . his third , he did not ; he said , ' said gandalf . 'but i wonder many other things 'safely giants . these morning , as very as , as so might in him , and designed he offered , and soon the night , the buckland was warm carried and by him . ' 'yes be , it is true that long a ring thing , ' said frodo , 'so so i have always it . and why not worry ? he asked softly . cheers ! ' he said with a commanding voice , and frodo a quick look and be him . ' 'i think it is\n"
     ]
    }
   ],
   "source": [
    "rslt4 = Fake_Old_Baggins(1000)\n",
    "print(rslt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to solve the problem with regression method, failed by get always the same words. Gradient vanishing found. Aborted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_group = 8\n",
    "random_range = int(len(words)/timestep_group) - 1\n",
    "\n",
    "def get_next_batch(batch_size):\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    index = np.random.randint(0,random_range,batch_size)\n",
    "    #print(\"index: \",index)\n",
    "    for i in range(batch_size):\n",
    "        temp = []\n",
    "        for j in range(timestep_group-1):\n",
    "            #print(j,\"Get word:\",words[index[i]+j])\n",
    "            temp.append(w2v.wv.get_vector(words[index[i]+j]))\n",
    "        X_batch.append(temp)\n",
    "        #print(\"Label is:\", words[index[i]+timestep_group-1])\n",
    "        Y_batch.append(w2v.wv.get_vector(words[index[i]+timestep_group-1]))\n",
    "    return np.array(X_batch),np.array(Y_batch) \n",
    "\n",
    "alpha = 0.0005\n",
    "input_size = emb_dim\n",
    "timestep_size = timestep_group - 1\n",
    "hidden_size = 512\n",
    "layer_num = 2 \n",
    "cell_type = \"lstm\"\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 7, emb_dim])\n",
    "y_input = tf.placeholder(tf.float32, [None, emb_dim])\n",
    "batch_size = tf.placeholder(tf.int32, [])\n",
    "keep_prob = tf.placeholder(tf.float32, [])\n",
    "\n",
    "def lstm_cell(cell_type,num_nodes,keep_prob):\n",
    "    # assert(cell_type in [\"lstm\",\"block_lstm\"],\"Wrong cell type\")\n",
    "    if cell_type == \"lstm\":\n",
    "        cell = rnn.BasicLSTMCell(num_nodes)\n",
    "    else:\n",
    "        cell = rnn.LSTMBlockCell(num_nodes)\n",
    "    cell = rnn.DropoutWrapper(cell,output_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "mlstm_cell = rnn.MultiRNNCell([lstm_cell(cell_type,hidden_size,keep_prob) for _ in range(layer_num)],state_is_tuple=True)\n",
    "init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "outputs = list()\n",
    "state = init_state\n",
    "\n",
    "with tf.variable_scope('RNN'):\n",
    "    for timestep in range(timestep_size):\n",
    "        (cell_output,state) = mlstm_cell(X[:,timestep,:],state)\n",
    "        outputs.append(cell_output)\n",
    "h_state = outputs[-1]\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([hidden_size,emb_dim],stddev=0.1),dtype=tf.float32)\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[emb_dim]),dtype=tf.float32)\n",
    "y_pre = tf.matmul(h_state,W) + bias\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_input,y_pre)\n",
    "train_op = tf.train.AdamOptimizer(alpha).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#time0 = time.time()\n",
    "for i in range(5000):\n",
    "    _batch_size=100\n",
    "    X_batch, y_batch = get_next_batch(_batch_size)\n",
    "    cost, _ = sess.run([loss,train_op],feed_dict={X: X_batch, y_input:y_batch,keep_prob:0.5, batch_size: _batch_size})\n",
    "    if (i+1)%10 == 0:\n",
    "        print(\"step {},train cost={:.6f}\".format(i+1,cost))\n",
    "\n",
    "#kickoff_phrase = [\"Gandalf\",\"is\",\"reading\",\"a\",\"riddle\",\"on\",\"a\"]\n",
    "kickoff_phrase = [\"is\",\"reading\",\"a\",\"riddle\",\"on\",\"a\",\"track\"]\n",
    "kickoff_vector = []\n",
    "for i in range(timestep_size):\n",
    "    kickoff_vector.append(w2v.wv.get_vector(kickoff_phrase[i]))\n",
    "kickoff_batch = []\n",
    "kickoff_batch.append(kickoff_vector)\n",
    "kickoff_batch = np.array(kickoff_batch)\n",
    "\n",
    "kickoff_batch.shape\n",
    "\n",
    "predict_vector = sess.run([y_pre],feed_dict={X: kickoff_batch, keep_prob:1.0,batch_size: 1})\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
